{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# ![](https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png) Introduction to Linear Regression - Part 2\n",
    "Week 3 | Lesson 3.02\n",
    "\n",
    "### LEARNING OBJECTIVES\n",
    "*After this lesson, you will be able to:*\n",
    "- List the assumptions for a MLR model.\n",
    "- Fit SLR and MLR models in `statsmodels`.\n",
    "\n",
    "---\n",
    "\n",
    "## SLR to MLR\n",
    "\n",
    "The TL;DR of multiple linear regression (MLR) is that, rather than using one predictor to predict an independent variable, we include others.\n",
    "\n",
    "## Assumptions of MLR\n",
    "Just like SLR, there are assumptions in MLR. Luckily, they're really similar to the SLR assumptions.\n",
    "- **Linearity:** $Y$ must have an approximately linear relationship with each independent $X_i$.\n",
    "- **Independence:** Errors (residuals) $\\varepsilon_i$ and $\\varepsilon_j$ must be independent of one another for any $i \\neq j$.\n",
    "- **Normality:** The errors (residuals) follow a Normal distribution.\n",
    "- **Equality of Variances:** The errors (residuals) should have a roughly consistent pattern, regardless of the value of the $X_i$. (There should be no discernable relationship between $X_i$ and the residuals.)\n",
    "- **Independence Part 2:** The independent variables $X_i$ and $X_j$ must be independent of one another for any $i \\neq j$.\n",
    "\n",
    "## Dummy Variables\n",
    "\n",
    "We briefly mentioned how to convert qualitative variables into \"dummy variables\" for use in Python. Let's touch on a caution moving forward and interpreting these. Let's head to [this link](https://chrisalbon.com/python/pandas_convert_categorical_to_dummies.html) together.\n",
    "\n",
    "If you convert a qualitative variable to dummy variables, you want to turn a variable with $p$ categories into $p-1$ variables.\n",
    "\n",
    "Suppose we're working with the variable \"sex\" with values \"M\" and \"F\". You include in your model one variable for \"sex = F\" which takes on 1 if sex is female and 0 if sex is not female. Rather than saying \"a one unit change in $X$,\" the coefficient associated with \"sex = F\" is interpreted as the average change in $Y$ when sex = F relative to when sex = M.\n",
    "\n",
    "Suppose we're modeling revenue at a bar for each of the days of the week. We might include six variables: \"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\". The coefficient for Monday is interpreted in the average change in revenue when \"day = Monday\" relative to \"day = Sunday.\" The coefficient for Tuesday is interpreted in the average change in revenue when \"day = Tuesday\" relative to \"day = Sunday\" and so on.\n",
    "\n",
    "**Check:** If we were to include all $p$ predictors, what do you think would happen?\n",
    "\n",
    "## Interaction Terms\n",
    "\n",
    "Sometimes we want to include two variables that are highly correlated with one another. This would violate the assumption of independence between $X_i$ and $X_j$, but by including an interaction term - that is, an additional variable $X_i \\times X_j$ - we are able to overcome this issue by explicitly modeling the dependence between the two variables.\n",
    "\n",
    "If your model includes an interaction term $X_i \\times X_j$ that is \"statistically significant,\" it is customary to include $X_i \\times X_j$, $X_i$, **and** $X_j$ in your final model - even if $X_i$ or $X_j$ are not statistically significant!\n",
    "\n",
    "## Inference\n",
    "\n",
    "We can conduct inference on the parameters. The \\texttt{statsmodels} library will be particularly helpful for this.\n",
    "\n",
    "**Check:** Let's refresh our minds about inference.\n",
    "- Statistical inference is when we use sample statistics to learn more about population parameters.\n",
    "- A point estimate is the value of a statistic, or a \"best guess\" for the true value of the parameter. (Call of Duty sniper rifle.)\n",
    "- A standard error is the standard deviation of a statistic and helps us to quantify the variability of our estimator.\n",
    "- A p-value is the probability that we get a statistic as extreme or more extreme if we re-ran the experiment. If our p-value is less than alpha, we reject our null hypothesis. Otherwise, we fail to reject the null hypothesis.\n",
    "- A confidence interval is a set of possible values for the parameter."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Regression with Statsmodels\n",
    "\n",
    "Let's investigate the housing dataset with linear regression. Here's the documentation for `statsmodels`:\n",
    "* statsmodels -- [linear regression](http://statsmodels.sourceforge.net/devel/examples/#regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to Statsmodels\n",
    "\n",
    "`statsmodels` is a python package that provides access to many useful statistical calculations and models such as linear regression. It has some advantages over `scikit-learn`, in particular easier access to various statistical aspects of linear regression.\n",
    "\n",
    "First let's load and explore our dataset, then we'll see how to use `statsmodels`. We'll use `sklearn` to provide the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn import datasets ## imported datasets from scikit-learn\n",
    "data = datasets.load_boston() ## loaded Boston dataset from datasets library\n",
    "\n",
    "print data.DESCR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take a minute to see what the data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Let's see what data. can show us.\n",
    "data.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`scikit-learn` has already split off the house value data into the target variable. Let's see how to build a linear regression. First let's put the data into a data frame for convenience, and do a quick check to see that everything loaded correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "\n",
    "# Put the target (housing value -- MEDV) in another DataFrame\n",
    "targets = pd.DataFrame(data.target, columns=[\"MEDV\"])\n",
    "\n",
    "# Take a look at the first few rows\n",
    "print df.head()\n",
    "print targets.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's fit a linear model to the data. First let's take a look at some of the variables we identified visually as being linked to house value, RM and LSTAT. Let's look at each individually and then both together.\n",
    "\n",
    "#### Note that `statsmodels` does not add a constant term by default, so you need to use `X = sm.add_constant(X)` if you want a constant term, where `X` is the name of your dataframe containing your input (independent) variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "\n",
    "X = df[\"RM\"] ## X usually means our input variables (or independent variables)\n",
    "y = targets[\"MEDV\"] ## Y usually means our output/dependent variable\n",
    "\n",
    "# Note the argument order\n",
    "model = sm.OLS(y, X).fit() ## sm.OLS(output, input)\n",
    "predictions = model.predict(X)\n",
    "\n",
    "# Print out the statistics\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the Coefficients\n",
    "\n",
    "Here the coefficient of 3.634 means that as the `RM` variable increases by 1, the predicted value of `MDEV` increases by 3.634.\n",
    "\n",
    "Let's plot the predictions versus the actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot the model\n",
    "plt.scatter(predictions, y, s=30, c='r', marker='+', zorder=10)\n",
    "plt.xlabel(\"Predicted Values from RM\")\n",
    "plt.ylabel(\"Actual Values MEDV\")\n",
    "plt.show()\n",
    "print \"MSE:\", model.mse_model ## mean squared error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check**: How does this plot relate to the model? In other words, how are the independent variable (RM) and dependent variable (\"MEDV\") incorporated?\n",
    "\n",
    "Let's try it with a constant term now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## With a constant\n",
    "\n",
    "X = df[\"RM\"]\n",
    "y = targets[\"MEDV\"]\n",
    "X = sm.add_constant(X) ## let's add an intercept (beta_0) to our model\n",
    "\n",
    "# Note the argument order\n",
    "model = sm.OLS(y, X).fit()\n",
    "predictions = model.predict(X)\n",
    "\n",
    "# Print out the statistics\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot the model\n",
    "plt.scatter(predictions, y, s=30, c='r', marker='+', zorder=10)\n",
    "plt.xlabel(\"Predicted Values from RM\")\n",
    "plt.ylabel(\"Actual Values MEDV\")\n",
    "plt.show()\n",
    "print \"MSE:\", model.mse_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting the Coefficients\n",
    "\n",
    "With the constant term the coefficients are different. Without a constant we are forcing our model to go through the origin, but now we have a y-intercept at -34.67. We also changed the slope of the `RM` regressor from 3.634 to 9.1021.\n",
    "\n",
    "Next let's try a different predictor, `LSTAT`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = df[[\"LSTAT\"]]\n",
    "y = targets[\"MEDV\"]\n",
    "\n",
    "# Note the difference in argument order\n",
    "model = sm.OLS(y, X).fit()\n",
    "predictions = model.predict(X)\n",
    "\n",
    "# Print out the statistics\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot the model\n",
    "plt.scatter(predictions, y, s=30, c='r', marker='+', zorder=10)\n",
    "plt.xlabel(\"Predicted Values from LSTAT\")\n",
    "plt.ylabel(\"Actual Values MEDV\")\n",
    "plt.show()\n",
    "print \"MSE:\", model.mse_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's fit a model using both `RM` and `LSTAT`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = df[[\"RM\", \"LSTAT\"]]\n",
    "y = targets[\"MEDV\"]\n",
    "\n",
    "model = sm.OLS(y, X).fit()\n",
    "predictions = model.predict(X)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot the model\n",
    "plt.scatter(predictions, y, s=30, c='r', marker='+', zorder=10)\n",
    "plt.xlabel(\"Predicted Values from RM and LSTAT\")\n",
    "plt.ylabel(\"Actual Values MEDV\")\n",
    "plt.show()\n",
    "print \"MSE:\", model.mse_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comparing the models\n",
    "\n",
    "A perfect fit would yield a straight line when we plot the predicted values versus the true values. We'll quantify the goodness of fit soon.\n",
    "\n",
    "### Exercise\n",
    "\n",
    "Run the fit on all the variables with `X = df`. Did this improve the fit versus the previously tested variable combinations? (Use mean squared error)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus\n",
    "\n",
    "We'll go over using Scikit-Learn later this week, but you can get a head start now by repeating some of the exercises using `sklearn` instead of `statsmodels`.\n",
    "\n",
    "### Exercises\n",
    "\n",
    "Recreate the model fits above with `scikit-learn`:\n",
    "* a model using LSTAT\n",
    "* a model using RM and LSTAT\n",
    "* a model using all the variables\n",
    "\n",
    "Compare the mean squared errors for each model between the two packages. Do they differ significantly? Why or why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
