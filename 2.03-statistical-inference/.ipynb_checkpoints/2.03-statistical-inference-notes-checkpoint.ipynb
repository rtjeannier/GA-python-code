{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![](https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png) Introduction to Statistics - Part 2\n",
    "Week 2 | Lesson 2.03\n",
    "\n",
    "### LEARNING OBJECTIVES\n",
    "*After this lesson, you will be able to:*\n",
    "- Generate confidence intervals and interpret the results.\n",
    "- Execute hypothesis tests and interpret the results.\n",
    "- State and apply the Central Limit Theorem.\n",
    "- Differentiate between parametric and nonparametric statistical tests.\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "We can organize most of statistics into two large sub-fields: **descriptive** statistics and **inferential** statistics.\n",
    "\n",
    "- **Descriptive statistics** focuses on summarizing, describing, and understanding data we observe.\n",
    "- **Inferential statistics** focuses on generalizing results from a sample to a larger population.\n",
    "\n",
    "For today, we're going to focus almost entirely on inferential statistics. We'll still rely on descriptive stats, but now we're taking the descriptive statistics and generalizing them!\n",
    "\n",
    "Our goal is to calculate sample statistics and then rely on properties of a random sample (and perhaps additional assumptions) to be able to make inferences that we generalize to the larger population of interest.\n",
    "\n",
    "We will also rely on the library scipy (`scipy.stats`) for much of our work today, so let's import it now `as stats`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normal Distribution\n",
    "**Discussion:** What do you know about the [Normal distribution](https://en.wikipedia.org/wiki/Normal_distribution) so far?\n",
    "\n",
    "The Normal distribution is one of the most commonly used distributions in all of statistics. Normality is an assumption that underlies many statistical tests and serves as a convenient model for the *distribution* of many (but not all!) variables.\n",
    "\n",
    "**Check:** What is a distribution?\n",
    "\n",
    "The Normal distribution is parameterized by two parameters: the population mean and the population standard deviation. If a variable follows a Normal distribution exactly, its mean, median, and mode will all be equal.\n",
    "\n",
    "**Check:** How is my Normal distribution different if I provide the population variance instead of the population standard deviation?\n",
    "\n",
    "Intelligence Quotient (IQ) follows a Normal distribution by design. IQ is Normally distributed with a mean of 100 and a standard deviation of 15. (We might say $IQ \\sim Normal(100,15)$ or $IQ \\sim N(100,15)$.)\n",
    "\n",
    "**Check:** When we discuss distributions, we generally care about three characteristics. What are these three characteristics and how are they reflected in $IQ \\sim N(100,15)$?\n",
    "\n",
    "### The 68-95-99.7 Rule\n",
    "It is often to our benefit to identify how extreme (or far away from the expected value) a particular observation is within the context of a distribution. For example, an extreme stock price might indicate a major shift in the market and thus might dictate a need to buy or sell. An extreme drop in air pressure might indicate a significant weather event, causing a reaction from the National Weather Service. Quantifying just how extreme a particular observation is from the expected value (a.k.a. population mean) may indicate a particular action we should take.\n",
    "\n",
    "It is possible to show that, for a Normal distribution, 68% of observations from a population will fall within $\\mu \\pm \\sigma$, that 95% of observations from a population will fall within $\\mu \\pm 2\\sigma$, and that 99.7% of observations from a population will fall within $\\mu \\pm 3\\sigma$.\n",
    "\n",
    "**Check:** What percentage of individuals have an IQ between 85 and 115?\n",
    "\n",
    "**Check:** What percentage of individuals have an IQ above 100?\n",
    "\n",
    "**Check:** What percentage of individuals have an IQ between 85 and 130?\n",
    "\n",
    "### Z-Score\n",
    "While it's nice to have this 68-95-99.7 rule, let's generalize this to calculate the $z$-score of an observation.\n",
    "\n",
    "$$ z_i = \\frac{x_i - \\mu_x}{\\sigma_x}$$\n",
    "\n",
    "This measures how many standard deviations an observation $x_i$ is from the population mean. Since $X \\sim N(\\mu,\\sigma)$, we can show that $Z \\sim (0,1)$. We call $Z$ the **standard normal distribution** because it has a mean of 0 and standard deviation of 1.\n",
    "\n",
    "**Check:** If X is not normal, but we calculate Z by standardizing X using the mean and standard deviation of X as above, is Z ~ N(0,1)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.22474487,  0.        ,  1.22474487])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "values = np.array([3,4,5])\n",
    "\n",
    "stats.zscore(values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Central Limit Theorem\n",
    "Normality underlies many of the inferential techniques that we seek to use. It is important for us to determine when Normality is a condition we've met.\n",
    "\n",
    "Consider the variable $X$. We can take a sample from this population of size $n$ and find the mean of that sample. Let's call this $\\bar{x}_1$. We can take another sample from this population, also of size $n$, and find the mean of that sample. Let's call this $\\bar{x}_2$. We can do this over and over until we've calculated the mean of every possible sample of size $n$. If we plotted every sample mean $\\bar{x}$ on a histogram, note that we have plotted every possible value of $\\bar{x}$ and how frequently we observe each of these values. Thus, we get another distribution! This is called \"the sampling distribution of $\\bar{X}$.\"\n",
    "\n",
    "This distribution, the sampling distribution of $\\bar{X}$, is *sometimes* Normally distributed.\n",
    "- If $X \\sim N(\\mu,\\sigma)$, then $\\bar{X}$ is exactly $N\\left(\\mu,\\frac{\\sigma}{\\sqrt{n}}\\right)$.\n",
    "- If $X$ is not Normally distributed (or is unknown), then $\\bar{X}$ is approximately $N\\left(\\mu,\\frac{\\sigma}{\\sqrt{n}}\\right)$ if the sample size $n$ is at least 30.\n",
    "\n",
    "#### Why do we care?\n",
    "If $\\bar{X}$ is Normally distributed, then we know how $\\bar{X}$ behaves and that the sample mean was drawn from a Normal distribution. We can then use the sample mean to conduct inference on the population mean!\n",
    "\n",
    "## Confidence Intervals\n",
    "Suppose you are playing some video game like Call of Duty or Halo. If you had an enemy who was invisible, would you rather have a rocket launcher or a sniper rifle?\n",
    "\n",
    "**A confidence interval describes a set of possible values for the parameter based on a statistic.** Confidence intervals will be centered at the point estimate and then include +/- a few standard errors. *(Standard error is just the standard deviation of a statistic.)*\n",
    "\n",
    "Thus, the structure of a confidence interval will be:\n",
    "\n",
    "$$[\\text{point estimate}] +/- [\\text{multiplier}]\\times[\\text{standard error}]$$\n",
    "\n",
    "We use our sample mean to estimate our population mean. This should make sense, right? Our best guess for the population mean is the sample mean.\n",
    "\n",
    "Thus, our sample mean is our \"point estimate.\" (Sniper's bullet.)\n",
    "\n",
    "Because we know that $\\bar{X}$ is Normally distributed with standard deviation $\\frac{\\sigma}{\\sqrt{n}}$, we have an estimate of the variability of sample means from sample to sample. As such, our \"standard error\" will be $\\frac{\\sigma}{\\sqrt{n}}$.\n",
    "\n",
    "Similarly, because we know that $\\bar{X}$ is Normally distributed, our \"multiplier\" for our standard error will come from the Normal distribution. \n",
    "\n",
    "- If we want to be 90% confident that the true mean lies in our confidence interval our multiplier should be 1.645.\n",
    "- If we want to be 95% confident that the true mean lies in our confidence interval our multiplier should be 1.96.\n",
    "- If we want to be 99% confident that the true mean lies in our confidence interval our multiplier should be 2.575.\n",
    "\n",
    "We call 90%, 95%, and 99% the *confidence level*.\n",
    "\n",
    "Putting it all together, our $z$-based confidence interval is $\\bar{x} \\pm z \\times \\frac{\\sigma}{\\sqrt{n}}$\n",
    "\n",
    "### Interpretation\n",
    "Suppose a 95% confidence interval for the mean number of burritos Matt eats in a week is $(2.5,5.5)$. There are two interpretations from this.\n",
    "- We are 95% **confident** that the true mean of number of burritos Matt eats in a week is between 2.5 and 5.5.\n",
    "- If we pulled 100 samples and constructed confidence intervals in the same manner, we expect that 95 of the intervals would contain the true mean of number of burritos Matt eats in a week.\n",
    "\n",
    "**Check:** What is the point estimate for this confidence interval? What is the multiplier? What is the standard error?\n",
    "\n",
    "### Let's do this in Python!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "conf_level = 0.9\n",
    "mean = 4\n",
    "sigma = 1.5/1.96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.7411834487616282, 5.2588165512383718)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats.norm.interval(conf_level, loc=mean, scale=sigma) ## n = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "stats.norm.interval(conf_level, loc=mean, scale=sigma/(n ** 0.5)) ## n > 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What if we don't know the population standard deviation?\n",
    "Note that we've always used $\\sigma$ in our $z$-score calculations - that's because we *assume* that we know the population standard deviation. If we don't have the population standard deviation (which we usually won't), we can generate confidence intervals using the $t$-distribution instead of the $z$-distribution. Confidence intervals using the $t$-distribution will follow similar rules, except that the $t$-distribution assumes that the population standard deviation is unknown. \n",
    "\n",
    "The structure of a confidence interval based on $t$ is $\\bar{x} \\pm t \\times \\frac{s}{\\sqrt{n}}$, where $s$ is the **sample** standard deviation and the $t$-multiplier changes based on both the confidence level and the sample size $n$.\n",
    "\n",
    "### Let's do this in Python!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from scipy.stats import t\n",
    "n = 11\n",
    "degrees_of_freedom = n - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3.5817773544896014, 4.4182226455103981)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Interval for any N:\n",
    "t.interval(conf_level, degrees_of_freedom, loc=mean, scale=sigma/(n ** 0.5))\n",
    "## Note that degrees_of_freedom should be equal to n-1, where n is the sample size!!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis Tests\n",
    "A hypothesis test is a way to learn more about a parameter of interest. It is an inversion of a confidence interval, in the sense that a confidence interval and hypothesis tests will provide identical results. A confidence interval conveys more information, but interpreting a hypothesis test is important.\n",
    "\n",
    "\n",
    "### Step 1: Construct a null hypothesis that you seek to contradict and its complement, the alternative hypothesis.\n",
    "Suppose you want to disprove that the mean number of burritos I eat in a week is equal to 4. Then your null hypothesis (H_0) and alternative (H_A) hypothesis are:\n",
    "- $H_0: \\mu = 4$\n",
    "- $H_A: \\mu \\neq 4$\n",
    "\n",
    "If you want to show that the mean number of burritos I eat in a week is **less than** 4, then your hypotheses are:\n",
    "- $H_0: \\mu \\geq 4$\n",
    "- $H_A: \\mu < 4$\n",
    "\n",
    "Your hypotheses must be mutually exclusive and include all possible values of the parameter. **What you want to show should be in your alternative hypothesis.**\n",
    "\n",
    "**Check:** Suppose you wanted to show that the mean number of burritos I eat in a week is **more than** 4. What are the hypotheses?\n",
    "\n",
    "### Step 2: Specify a level of significance.\n",
    "You may have heard \"alpha equals point oh-five!\" before. A lower case alpha ($\\alpha$) is used to denote level of significance (or, more cryptically, Type I error). It is the probability of rejecting the null hypothesis given that the null hypothesis is actually true. Canonical levels are 0.01, 0.05, and 0.10. A higher alpha level means that you are likelier to reject your null hypothesis, but this makes it likelier that you **erroneously** reject your null hypothesis. The most common level is 0.05, but check your field's standards!\n",
    "\n",
    "### Step 3: Calculate your point estimate.\n",
    "In this case, your point estimate will simply be your sample mean - this should be easy!\n",
    "\n",
    "### Step 4: Calculate your test statistic.\n",
    "In this case, your test statistic will be $z = \\frac{\\bar{x} - \\mu}{\\sigma/\\sqrt{n}}$ - should still be easy!\n",
    "\n",
    "### Step 5: Find your $p$-value.\n",
    "The definition of $p$-value is \"the probability that, given a re-run of your experiment, you get a test statistic that is as extreme or more extreme than the test statistic you just received.\" Within the context of what we just did in Step 4, your $p$-value indicates that a re-run of the experiment would yield a $z$-score that is as extreme or more extreme than the one you just got.\n",
    "\n",
    "This is a measure of how extreme our experiment's results are. This should make sense - remember above how we talked about quantifying how far observations are from their expected value (a.k.a. population mean). This is the same thing! We're quantifying how far our sample statistic (a summary of our sample observations) is from our expected value of the statistic by seeing how many standard errors (standard deviations) we are from the expected value.\n",
    "\n",
    "- If that $p$-value is less than your pre-determined $\\alpha$, then you can reject your null hypothesis and conclude that your alternative hypothesis is indeed correct.\n",
    "- If that $p$-value is more than your pre-determined $\\alpha$, then you **fail to reject** your null hypothesis and **cannot conclude that either the null or the alternative is correct**.\n",
    "- If that $p$-value is equal to your pre-determined $\\alpha$, then your results are inconclusive. Start a brand new study over or assume that you cannot reject your null hypothesis.\n",
    "\n",
    "[Let's do this in Python!](https://docs.scipy.org/doc/scipy-0.14.0/reference/generated/scipy.stats.ttest_1samp.html)\n",
    "\n",
    "### Discussion of p-hacking.\n",
    "\n",
    "### But what about proportions or categorical data?\n",
    "There are certainly other things we can look into - testing whether two proportions are equal, testing whether one mean is greater than another, testing whether or not there is an association between two categorical variables, testing whether your correlation is significantly greater than zero or not. We simply don't have time to go through every individual statistical test.\n",
    "\n",
    "If you think a statistical test is appropriate, think about what you seek to show, set up your hypotheses, and then go hunting online for the proper test. The SciPy documentation is incredibly robust and there are myriad statistical sites that will help you to identify the proper test. (I'm sure that your classmates and instructors would be willing to help as well!)\n",
    "\n",
    "### Note on Nonparametric Statistics\n",
    "Thus far, all of our tests have been **parametric.** That is, we have assumed a certain distribution for our data. However, there are alternatives in the case where we cannot assume a particular distribution for our data. When we make no assumptions about the distribution for our data, we call our data **nonparametric**. For nearly every parametric test, there is a nonparametric analog available.\n",
    "\n",
    "## Conclusion\n",
    "- Q & A\n",
    "- Lab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_1samp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "burritos = [5,6,4,7,4,5,6,5,5,5,5,5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_1sampResult(statistic=4.8409485236568743, pvalue=0.00051820682581846722)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ttest_1samp(burritos,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
