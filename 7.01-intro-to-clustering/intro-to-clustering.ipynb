{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Intro to Clustering and K-Means\n",
    "\n",
    "### Learning Objectives: By the end of this lesson students will be able to:\n",
    "\n",
    "- Explain the difference between supervised and unsupervised learning problems\n",
    "- Explain the algorithm behind k-means clustering\n",
    "- Identify some shortcomings of k-means\n",
    "- Implement k-means clustering in python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a classification problem?\n",
    "### Describe one classification algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "from IPython.core.display import HTML "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What's the difference between supervised and unsupervised learning problems?\n",
    "\n",
    "- Supervised Learning: We have, as part of our training data, observed Y values.\n",
    "- Unsupervised Learning: We have, as part of our training data, *no* observed Y values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "![](http://oliviaklose.azurewebsites.net/content/images/2015/02/2-supervised-vs-unsupervised-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### SUPERVISED LEARNING\n",
    "**Regression** - Create a model to describe the relationship between X and a continuous Y.\n",
    "\n",
    "**Classification** - Create a model to describe the relationship between X and a discrete Y.\n",
    "\n",
    "### UNSUPERVISED LEARNING\n",
    "**Clustering** - Find groups that already exist in the data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Check: Let's come up with examples for each of these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Helpful uses for clustering: \n",
    "   - Find items with similar behavior (users, products, voters, etc)\n",
    "   - Market segmentation\n",
    "   - Understand complex systems\n",
    "   - Discover meaningful categories for your data\n",
    "   - Reduce the number of classes by grouping (e.g. bourbons, scotches -> whiskeys)\n",
    "   - Reduce the dimensions of your problem\n",
    "   - Pre-processing! Create labels for supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Clustering is incredibly useful. Much like regression and classification use cases became apparent over time, opportunities for clustering will become more obvious as we encounter more examples.\n",
    "\n",
    "# Much like we talked through NLP, how can we get a computer to algorithmically tell which groups are different?\n",
    "\n",
    "![](http://home.deib.polimi.it/matteucc/Clustering/tutorial_html/images/clustering.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "# Hey. Don't scroll ahead. \n",
    "![](http://images-cdn.9gag.com/photo/aq2m1mY_700b.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# K Means\n",
    "\n",
    "A *partitional* clustering algorithm that assigngs clusters based distance from a centroid.\n",
    "\n",
    "Partitional: type of clustering that determines all clusters at once, as opposed to hierarchical clustering methods.\n",
    "\n",
    "\n",
    "### My First<sup>TM</sup> Clustering Algorithm\n",
    "\n",
    "1. Pick a value for k (the number of clusters to create)\n",
    "2. Initialize k 'centroids' (starting points) in your data\n",
    "3. Create your clusters. Assign each point to the nearest centroid. \n",
    "4. Make your clusters better. Move each centroid to the center of its cluster. \n",
    "5. Repeat steps 3-4 until your centroids converge. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# K Means\n",
    "\n",
    "1. Pick a value for k (the number of clusters to create)\n",
    "\n",
    "![](assets/images/kmeans1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# K Means\n",
    "\n",
    "1. Pick a value for k (the number of clusters to create)\n",
    "2. Initialize k 'centroids' (starting points) in your data\n",
    "\n",
    "![](assets/images/kmeans2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# K Means\n",
    "\n",
    "1. Pick a value for k (the number of clusters to create)\n",
    "2. Initialize k 'centroids' (starting points) in your data\n",
    "3. Assign each point to the nearest centroid. These are your 'clusters'.\n",
    "\n",
    "![](assets/images/kmeans3.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# K Means\n",
    "\n",
    "1. Pick a value for k (the number of clusters to create)\n",
    "2. Initialize k 'centroids' (starting points) in your data\n",
    "3. Assign each point to the nearest centroid. These are your clusters.\n",
    "4. Make your clusters better. Move each centroid to the center of its cluster. \n",
    "\n",
    "![](assets/images/kmeans4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# K Means\n",
    "\n",
    "1. Pick a value for k (the number of clusters to create)\n",
    "2. Initialize k 'centroids' (starting points) in your data\n",
    "3. Assign each point to the nearest centroid. These are your clusters.\n",
    "4. Make your clusters better. Move each centroid to the center of its cluster. \n",
    "5. Repeat steps 3-4 until your centroids converge. \n",
    "\n",
    "![](assets/images/kmeans5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# K Means\n",
    "\n",
    "1. Pick a value for k (the number of clusters to create)\n",
    "2. Initialize k 'centroids' (starting points) in your data\n",
    "3. Assign each point to the nearest centroid. These are your clusters.\n",
    "4. Make your clusters better. Move each centroid to the center of its cluster. \n",
    "5. Repeat steps 3-4 until your centroids converge. \n",
    "\n",
    "![](assets/images/kmeans6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# K Means\n",
    "\n",
    "1. Pick a value for k (the number of clusters to create)\n",
    "2. Initialize k 'centroids' (starting points) in your data\n",
    "3. Assign each point to the nearest centroid. These are your clusters.\n",
    "4. Make your clusters better. Move each centroid to the center of its cluster. \n",
    "5. Repeat steps 3-4 until your centroids converge. \n",
    "\n",
    "![](assets/images/kmeans7.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# K Means\n",
    "\n",
    "1. Pick a value for k (the number of clusters to create)\n",
    "2. Initialize k 'centroids' (starting points) in your data\n",
    "3. Assign each point to the nearest centroid. These are your clusters.\n",
    "4. Make your clusters better. Move each centroid to the center of its cluster. \n",
    "5. Repeat steps 3-4 until your centroids converge. \n",
    "\n",
    "![](assets/images/kmeans8.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# K Means\n",
    "\n",
    "1. Pick a value for k (the number of clusters to create)\n",
    "2. Initialize k 'centroids' (starting points) in your data\n",
    "3. Assign each point to the nearest centroid. These are your clusters.\n",
    "4. Make your clusters better. Move each centroid to the center of its cluster. \n",
    "5. Repeat steps 3-4 until your centroids converge. \n",
    "\n",
    "![](assets/images/kmeans9.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# K Means\n",
    "\n",
    "1. Pick a value for k (the number of clusters to create)\n",
    "2. Initialize k 'centroids' (starting points) in your data\n",
    "3. Assign each point to the nearest centroid. These are your clusters.\n",
    "4. Make your clusters better. Move each centroid to the center of its cluster. \n",
    "5. Repeat steps 3-4 until your centroids converge. \n",
    "\n",
    "![](assets/images/kmeans10.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's explore this excellent visualization:\n",
    "https://www.naftaliharris.com/blog/visualizing-k-means-clustering/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "\n",
    "These tutorial images come from Andrew Moore's CS class at Carnegie Mellon. His slide deck is online here: https://www.autonlab.org/_media/tutorials/kmeans11.pdf. (There are additional tutorials at https://www.autonlab.org.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### k-Means is a powerful algorithm, but different starting points may give you different clusters. You won't necessarily get an optimal cluster. \n",
    "\n",
    "### What changes could we make to the starting points to improve the algorithm?\n",
    "\n",
    "\n",
    "(A link to an improved algorithm to initializing centroids for k-means is [here](https://pdfs.semanticscholar.org/b278/74148c4af4d3eedc64909b0b738e5b1c73cf.pdf)). This is the default method for sklearn's clustering.\n",
    "\n",
    "### Let's practice a toy example by hand. Take a 1-dimensional array of numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "arr = [2, 5, 6, 8, 12, 15, 18, 28, 30, 36, 40]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Pick `k = 3` random start centroids and let the other points fall into clusters based on the closest centroid. Then, reassign your centroids based on the mean value of your cluster and repeat the process. \n",
    "\n",
    "Check with your neighbors. Do you have the same clusters? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Metrics for k-Means\n",
    "\n",
    "**Inertia** -- sum of squared errors for each cluster (http://scikit-learn.org/stable/modules/clustering.html#k-means)\n",
    "- Low inertia = dense cluster\n",
    "\n",
    "**Silhouette Score** -- measure of how far apart clusters are (http://scikit-learn.org/stable/modules/clustering.html#silhouette-coefficient)\n",
    "- High Silhouette Score = clusters are well-separated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shortcomings:\n",
    "\n",
    "What are some potential problems with k means clustering?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How do I know which k to pick?\n",
    "\n",
    "Sometimes you have good context:\n",
    "   - I need to create 14 profiles for marketing to target\n",
    "\n",
    "Other times you have to figure it out:\n",
    "   - My scatter plots show 2 linearly separable clusters\n",
    "\n",
    "More on this later this morning. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A note on binaries/categorical data:\n",
    "\n",
    "When our dataset includes binary variables, it assumes continous numerical variables. Euclidian distance is not always a useful measure when you're dealing with binary data. There are other clustering algorithms we will learn later this week that are better for categorical data.\n",
    "\n",
    "That said, you can use k means on categorical data if you are aware of its shortcomings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Some helpful links on the subject:\n",
    "- http://stats.stackexchange.com/questions/40613/why-dont-dummy-variables-have-the-continuous-adjacent-category-problem-in-clust\n",
    "- http://stats.stackexchange.com/questions/174556/k-means-clustering-with-dummy-variables\n",
    "- http://datascience.stackexchange.com/questions/22/k-means-clustering-for-mixed-numeric-and-categorical-data\n",
    "- http://www-01.ibm.com/support/docview.wss?uid=swg21477401"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Group Practice\n",
    "Let's do some clustering with the iris dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn import cluster, datasets, preprocessing, metrics\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "plt.style.use('fivethirtyeight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"assets/datasets/iris.csv\")\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "X = df[['SepalLength','SepalWidth','PetalLength','PetalWidth']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now let's plot the data to see the distributions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cols = X.columns[:-1]\n",
    "sns.pairplot(X[cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Let's check out the distributions of our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What are some concerns that we might have here?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_scaled = preprocessing.normalize(X,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_scaled, columns=X.columns).describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Now that we've formatted our data and understand it's structures, we can finally go ahead and cluster.\n",
    "\n",
    "We're going to set k at two given the behavior we were seeing above in our graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "kmeans = cluster.KMeans(n_clusters=k)\n",
    "kmeans.fit(X_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can use sklearn's built-in functions to determine the locations of the labels, centroids, and cluster inertia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "labels = kmeans.labels_\n",
    "centroids = kmeans.cluster_centers_\n",
    "inertia = kmeans.inertia_\n",
    "\n",
    "print 'Labels:', labels\n",
    "print ''\n",
    "print 'Centroids:', centroids\n",
    "print ''\n",
    "print 'Inertia:', inertia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And to compute the clusters' silhouette coefficient:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print 'Silhouette Score:', metrics.silhouette_score(X_scaled, labels, metric='euclidean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "...and we're done! You've completed your first clustering analysis.\n",
    "\n",
    "Let's see how it looks. First, let's put the labels columns into our dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'] = labels\n",
    "X['label'] = labels\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Let's plot each cluster in a different color. Seaborn has a 'hue' parameter we can use for this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = X.columns[:-1]\n",
    "sns.pairplot(X, x_vars=cols, y_vars= cols, hue='label')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Just for comparison, here's the data colored by name of the plant. \n",
    "\n",
    "Go back up and change the number of clusters to 3, or 4. See how those clusters compare to the data below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "sns.pairplot(df, x_vars=cols, y_vars= cols, hue='Name')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Wrap Up Multiple Choice\n",
    "\n",
    "### Which of the following is an unsupervised learning problem?\n",
    "\n",
    "A. Predicting a store's sales.\n",
    "\n",
    "B. Grouping stores by state.\n",
    "\n",
    "C. Grouping stores by demographic profiles.\n",
    "\n",
    "### How might we find k for a k-Means clustering problem?\n",
    "A. Treat k as a tuning parameter and use some cross-validation to find the most appropriate value of k.\n",
    "\n",
    "B. Rely on subject-matter expertise to recognize how many clusters to create.\n",
    "\n",
    "C. Visually examine scatterplots.\n",
    "\n",
    "### What are drawbacks to the k-means method? (Plot twist; not a multiple choice question!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Resources\n",
    "\n",
    "\n",
    "- [Sci-Kit Learn Clustering Overview](http://scikit-learn.org/stable/modules/clustering.html#k-means)\n",
    "- [SKLearn K-Means Documentation](http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans)\n",
    "- [SKLearn Clustering Code - See _k_init__ for explanation of k++ centroid selection](https://github.com/scikit-learn/scikit-learn/blob/51a765a/sklearn/cluster/k_means_.py#L769)\n",
    "- [Clustering Tutorial](http://home.deib.polimi.it/matteucc/Clustering/tutorial_html/)\n",
    "- [Wikipedia's Deep Dive on Clustering](https://en.wikipedia.org/wiki/K-means_clustering)\n",
    "- [An excellent blog from Galvanize on K-Means Clustering](http://www.galvanize.com/blog/introduction-k-means-cluster-analysis/#.V9aXqpOAOkp)"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
