{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to Natural Language Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we've covered three ways to use natural language processing to extract text features from data.\n",
    "\n",
    "- Count Vectorizer\n",
    "- Hashing Vectorizer\n",
    "- TF-IDF\n",
    "\n",
    "Now we'll go over the implementation in code.\n",
    "\n",
    "Each of the methods has two steps - a fit and transform. The **fit** teaches the function the vocabulary, **transform** applies that vocabulary to the selected text.\n",
    "\n",
    "These models are all 'bag of words' models. What does this mean?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#let's load some sample data:\n",
    "\n",
    "spam = 'Hello,\\nI saw your contact information on LinkedIn. I have carefully read through your profile and you seem to have an outstanding personality. This is one major reason why I am in contact with you. My name is Mr. Valery Grayfer Chairman of the Board of Directors of PJSC \"LUKOIL\". I am 86 years old and I was diagnosed with cancer 2 years ago. I will be going in for an operation later this week. I decided to WILL/Donate the sum of 8,750,000.00 Euros(Eight Million Seven Hundred And Fifty Thousand Euros Only etc. etc.'\n",
    "\n",
    "ham = 'Hello,\\nI am writing in regards to your application to the position of Data Scientist at Hooli X. We are pleased to inform you that you passed the first round of interviews and we would like to invite you for an on-site interview with our Senior Data Scientist Mr. John Smith. You will find attached to this message further information on date, time and location of the interview. Please let me know if I can be of any further assistance. Best Regards.'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello,\n",
      "I saw your contact information on LinkedIn. I have carefully read through your profile and you seem to have an outstanding personality. This is one major reason why I am in contact with you. My name is Mr. Valery Grayfer Chairman of the Board of Directors of PJSC \"LUKOIL\". I am 86 years old and I was diagnosed with cancer 2 years ago. I will be going in for an operation later this week. I decided to WILL/Donate the sum of 8,750,000.00 Euros(Eight Million Seven Hundred And Fifty Thousand Euros Only etc. etc.\n",
      "\n",
      "Hello,\n",
      "I am writing in regards to your application to the position of Data Scientist at Hooli X. We are pleased to inform you that you passed the first round of interviews and we would like to invite you for an on-site interview with our Senior Data Scientist Mr. John Smith. You will find attached to this message further information on date, time and location of the interview. Please let me know if I can be of any further assistance. Best Regards.\n"
     ]
    }
   ],
   "source": [
    "print spam\n",
    "print\n",
    "print ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
       "        strip_accents=None, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
       "        tokenizer=None, vocabulary=None)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's apply a count vectorizer:\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, HashingVectorizer\n",
    "\n",
    "#instantiate our model\n",
    "cvec = CountVectorizer()\n",
    "\n",
    "#fit the count vectorizer to the data. This 'teaches' the count vectorizer the dictionary.\n",
    "cvec.fit([spam])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to generate features we have to transform the data after we fit the count vecotrizer.\n",
    "cvecdata= cvec.transform([spam])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we can turn our features into a dataframe:\n",
    "\n",
    "df  = pd.DataFrame(cvecdata.todense(),\n",
    "             columns=cvec.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>750</th>\n",
       "      <th>86</th>\n",
       "      <th>ago</th>\n",
       "      <th>am</th>\n",
       "      <th>an</th>\n",
       "      <th>and</th>\n",
       "      <th>be</th>\n",
       "      <th>board</th>\n",
       "      <th>...</th>\n",
       "      <th>to</th>\n",
       "      <th>valery</th>\n",
       "      <th>was</th>\n",
       "      <th>week</th>\n",
       "      <th>why</th>\n",
       "      <th>will</th>\n",
       "      <th>with</th>\n",
       "      <th>years</th>\n",
       "      <th>you</th>\n",
       "      <th>your</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows Ã— 69 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   00  000  750  86  ago  am  an  and  be  board  ...   to  valery  was  week  \\\n",
       "0   1    1    1   1    1   2   2    3   1      1  ...    2       1    1     1   \n",
       "\n",
       "   why  will  with  years  you  your  \n",
       "0    1     2     2      2    2     2  \n",
       "\n",
       "[1 rows x 69 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hashing Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "hvec = HashingVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#your turn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>479532</th>\n",
       "      <th>144749</th>\n",
       "      <th>174171</th>\n",
       "      <th>832412</th>\n",
       "      <th>828689</th>\n",
       "      <th>994433</th>\n",
       "      <th>1005907</th>\n",
       "      <th>170062</th>\n",
       "      <th>675997</th>\n",
       "      <th>959146</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.338062</td>\n",
       "      <td>0.169031</td>\n",
       "      <td>0.169031</td>\n",
       "      <td>0.169031</td>\n",
       "      <td>0.169031</td>\n",
       "      <td>0.169031</td>\n",
       "      <td>0.169031</td>\n",
       "      <td>0.169031</td>\n",
       "      <td>0.169031</td>\n",
       "      <td>0.084515</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    479532    144749    174171    832412    828689    994433    1005907  \\\n",
       "0  0.338062  0.169031  0.169031  0.169031  0.169031  0.169031  0.169031   \n",
       "\n",
       "    170062    675997    959146   \n",
       "0  0.169031  0.169031  0.084515  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df  = pd.DataFrame(hvec.transform([spam]).todense())\n",
    "df.transpose().sort_values(0, ascending=False).head(10).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf-idf Document Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
       "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm=u'l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words='english', strip_accents=None, sublinear_tf=False,\n",
       "        token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "tvec = TfidfVectorizer(stop_words='english')\n",
    "tvec.fit([spam, ham])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df  = pd.DataFrame(tvec.transform([spam, ham]).todense(),\n",
    "                   columns=tvec.get_feature_names(),\n",
    "                   index=['spam', 'ham'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>00</th>\n",
       "      <th>000</th>\n",
       "      <th>750</th>\n",
       "      <th>86</th>\n",
       "      <th>ago</th>\n",
       "      <th>application</th>\n",
       "      <th>assistance</th>\n",
       "      <th>attached</th>\n",
       "      <th>best</th>\n",
       "      <th>board</th>\n",
       "      <th>...</th>\n",
       "      <th>seven</th>\n",
       "      <th>site</th>\n",
       "      <th>smith</th>\n",
       "      <th>sum</th>\n",
       "      <th>thousand</th>\n",
       "      <th>time</th>\n",
       "      <th>valery</th>\n",
       "      <th>week</th>\n",
       "      <th>writing</th>\n",
       "      <th>years</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <td>0.145067</td>\n",
       "      <td>0.145067</td>\n",
       "      <td>0.145067</td>\n",
       "      <td>0.145067</td>\n",
       "      <td>0.145067</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.145067</td>\n",
       "      <td>...</td>\n",
       "      <td>0.145067</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.145067</td>\n",
       "      <td>0.145067</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.145067</td>\n",
       "      <td>0.145067</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.290133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ham</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.155195</td>\n",
       "      <td>0.155195</td>\n",
       "      <td>0.155195</td>\n",
       "      <td>0.155195</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.155195</td>\n",
       "      <td>0.155195</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.155195</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.155195</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows Ã— 68 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            00       000       750        86       ago  application  \\\n",
       "spam  0.145067  0.145067  0.145067  0.145067  0.145067     0.000000   \n",
       "ham   0.000000  0.000000  0.000000  0.000000  0.000000     0.155195   \n",
       "\n",
       "      assistance  attached      best     board    ...        seven      site  \\\n",
       "spam    0.000000  0.000000  0.000000  0.145067    ...     0.145067  0.000000   \n",
       "ham     0.155195  0.155195  0.155195  0.000000    ...     0.000000  0.155195   \n",
       "\n",
       "         smith       sum  thousand      time    valery      week   writing  \\\n",
       "spam  0.000000  0.145067  0.145067  0.000000  0.145067  0.145067  0.000000   \n",
       "ham   0.155195  0.000000  0.000000  0.155195  0.000000  0.000000  0.155195   \n",
       "\n",
       "         years  \n",
       "spam  0.290133  \n",
       "ham   0.000000  \n",
       "\n",
       "[2 rows x 68 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are some shortcomings of the way we've created features from words so far?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question: How would we adjust to accomodate different words with the same or similar meanings. For example:\n",
    "\n",
    "- is, are, am\n",
    "- wolf, wolves\n",
    "- dance, dancing\n",
    "\n",
    "Our current process will count those words separately, when we might not necessarily want it to.\n",
    "\n",
    "We can use a library called nltk, or natural langauge toolkit, to preprocess our data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'cat'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize(\"cats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'goose'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('geese')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'interview'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer.lemmatize('interviews')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Before we can lemmitize our spam string we need to tokenize it.\n",
    "\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "#What is our tokenizer doing? Is anyone familiar with regex?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "spam_tokens = tokenizer.tokenize(spam.lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello',\n",
       " 'i',\n",
       " 'saw',\n",
       " 'your',\n",
       " 'contact',\n",
       " 'information',\n",
       " 'on',\n",
       " 'linkedin',\n",
       " 'i',\n",
       " 'have',\n",
       " 'carefully',\n",
       " 'read',\n",
       " 'through',\n",
       " 'your',\n",
       " 'profile',\n",
       " 'and',\n",
       " 'you',\n",
       " 'seem',\n",
       " 'to',\n",
       " 'have',\n",
       " 'an',\n",
       " 'outstanding',\n",
       " 'personality',\n",
       " 'this',\n",
       " 'is',\n",
       " 'one',\n",
       " 'major',\n",
       " 'reason',\n",
       " 'why',\n",
       " 'i',\n",
       " 'am',\n",
       " 'in',\n",
       " 'contact',\n",
       " 'with',\n",
       " 'you',\n",
       " 'my',\n",
       " 'name',\n",
       " 'is',\n",
       " 'mr',\n",
       " 'valery',\n",
       " 'grayfer',\n",
       " 'chairman',\n",
       " 'of',\n",
       " 'the',\n",
       " 'board',\n",
       " 'of',\n",
       " 'directors',\n",
       " 'of',\n",
       " 'pjsc',\n",
       " 'lukoil',\n",
       " 'i',\n",
       " 'am',\n",
       " '86',\n",
       " 'years',\n",
       " 'old',\n",
       " 'and',\n",
       " 'i',\n",
       " 'was',\n",
       " 'diagnosed',\n",
       " 'with',\n",
       " 'cancer',\n",
       " '2',\n",
       " 'years',\n",
       " 'ago',\n",
       " 'i',\n",
       " 'will',\n",
       " 'be',\n",
       " 'going',\n",
       " 'in',\n",
       " 'for',\n",
       " 'an',\n",
       " 'operation',\n",
       " 'later',\n",
       " 'this',\n",
       " 'week',\n",
       " 'i',\n",
       " 'decided',\n",
       " 'to',\n",
       " 'will',\n",
       " 'donate',\n",
       " 'the',\n",
       " 'sum',\n",
       " 'of',\n",
       " '8',\n",
       " '750',\n",
       " '000',\n",
       " '00',\n",
       " 'euros',\n",
       " 'eight',\n",
       " 'million',\n",
       " 'seven',\n",
       " 'hundred',\n",
       " 'and',\n",
       " 'fifty',\n",
       " 'thousand',\n",
       " 'euros',\n",
       " 'only',\n",
       " 'etc',\n",
       " 'etc']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokens_lem = [lemmatizer.lemmatize(i) for i in spam_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['hello',\n",
       " 'i',\n",
       " 'saw',\n",
       " 'your',\n",
       " 'contact',\n",
       " 'information',\n",
       " 'on',\n",
       " 'linkedin',\n",
       " 'i',\n",
       " 'have',\n",
       " 'carefully',\n",
       " 'read',\n",
       " 'through',\n",
       " 'your',\n",
       " 'profile',\n",
       " 'and',\n",
       " 'you',\n",
       " 'seem',\n",
       " 'to',\n",
       " 'have',\n",
       " 'an',\n",
       " 'outstanding',\n",
       " 'personality',\n",
       " 'this',\n",
       " 'is',\n",
       " 'one',\n",
       " 'major',\n",
       " 'reason',\n",
       " 'why',\n",
       " 'i',\n",
       " 'am',\n",
       " 'in',\n",
       " 'contact',\n",
       " 'with',\n",
       " 'you',\n",
       " 'my',\n",
       " 'name',\n",
       " 'is',\n",
       " 'mr',\n",
       " 'valery',\n",
       " 'grayfer',\n",
       " 'chairman',\n",
       " 'of',\n",
       " 'the',\n",
       " 'board',\n",
       " 'of',\n",
       " u'director',\n",
       " 'of',\n",
       " 'pjsc',\n",
       " 'lukoil',\n",
       " 'i',\n",
       " 'am',\n",
       " '86',\n",
       " u'year',\n",
       " 'old',\n",
       " 'and',\n",
       " 'i',\n",
       " u'wa',\n",
       " 'diagnosed',\n",
       " 'with',\n",
       " 'cancer',\n",
       " '2',\n",
       " u'year',\n",
       " 'ago',\n",
       " 'i',\n",
       " 'will',\n",
       " 'be',\n",
       " 'going',\n",
       " 'in',\n",
       " 'for',\n",
       " 'an',\n",
       " 'operation',\n",
       " 'later',\n",
       " 'this',\n",
       " 'week',\n",
       " 'i',\n",
       " 'decided',\n",
       " 'to',\n",
       " 'will',\n",
       " 'donate',\n",
       " 'the',\n",
       " 'sum',\n",
       " 'of',\n",
       " '8',\n",
       " '750',\n",
       " '000',\n",
       " '00',\n",
       " u'euro',\n",
       " 'eight',\n",
       " 'million',\n",
       " 'seven',\n",
       " 'hundred',\n",
       " 'and',\n",
       " 'fifty',\n",
       " 'thousand',\n",
       " u'euro',\n",
       " 'only',\n",
       " 'etc',\n",
       " 'etc']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens_lem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "paired = zip(spam_tokens, tokens_lem)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "paired"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How much of our text actually changed?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stemming is similar to lemmization, but it reduces topically similar words to their \"root.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create p_stemmer of class PorterStemmer\n",
    "p_stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stem_spam = [p_stemmer.stem(i) for i in spam_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "paired_stem = zip(spam_tokens, stem_spam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "paired_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What can we do with our results?\n",
    "It's now in the form of a list, so we can't plug it directly back in to the countvectorizer, but we can turn it back into a string if we wanted.\n",
    "\n",
    "We can use this information for other purposes though....\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Follow-up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How would we apply this information to a classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
