{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# ![](https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png) Ensemble Methods - Random Forests\n",
    "Week 6 | Lesson 6.02\n",
    "\n",
    "### LEARNING OBJECTIVES\n",
    "*After this lesson, you will be able to:*\n",
    "\n",
    "- Explain and implement bagging\n",
    "- Explain what a Random Forest is and how it is different from bagging of decision trees\n",
    "- Explain what Extra Trees models are\n",
    "- Apply both techniques to classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Opening\n",
    "\n",
    "**Recall:** What is bootstrapping?\n",
    "\n",
    "**Recall:** What is bagging?\n",
    "\n",
    "Today we will learn about random forests, which are essentially a variation of the bagging + decision tree model. This variation is a powerful tweak on decision trees, the specifics of which we'll get into shortly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Random Forests\n",
    "\n",
    "As we have seen, decision trees are very powerful machine learning models. Random forests, a step beyond decision trees are very widely used classifiers and regressors. They are relatively simple to use because they require very few parameters to set and they perform pretty well. Glossing over the finer details, for right now we'll think of random forests as bagged decision trees. (We'll see shortly that this isn't precisely correct, but that this is a good way to think about random forests for now.)\n",
    "\n",
    "**Check:** What are the main advantages of decision trees?\n",
    "\n",
    "On the other hand decision trees have some limitations. In particular, trees that are grown very deep tend to learn highly irregular patterns (a.k.a. they overfit their training sets). Bagging (bootstrap aggregating) helps to mitigate this problem by exposing different trees to different sub-samples of the whole training set.\n",
    "\n",
    "![](https://qph.ec.quoracdn.net/main-qimg-69838e874a74b9537c2540c65dbea725)\n",
    "\n",
    "Random forests are a method of further averaging multiple deep decision trees, trained on different parts of the same training set, with the goal of reducing the variance. This comes at the expense of a small increase in the bias and some loss of interpretability, but generally greatly boosts the performance of the final model.\n",
    "\n",
    "![www.cse.buffalo.edu/~jing/sdm10ensemble.htm](./assets/images/Ensemble.png)\n",
    "\n",
    "**Check:** Describe how the bagging algorithm works.\n",
    "\n",
    "_Random forests_ differ from bagging decision trees in only one way: they use a modified tree learning algorithm that selects, at each candidate split in the learning process, a **random subset of the features**. This process is sometimes called _feature bagging_. \n",
    "\n",
    "**Why might we do this?**\n",
    "\n",
    "The reason for doing this is the correlation of the trees in an ordinary bootstrap sample: if one or a few features are very strong predictors for the response variable (target output), these features will be selected in many of the bagging base trees, causing them to become correlated. By selecting a random subset of the features at each split, we counter this correlation between base trees, strengthening the overall model.\n",
    "\n",
    "For a problem with $p$ features, it is typical to use:\n",
    "- $\\sqrt{p}$ (rounded down) features in each split for a classification problem.\n",
    "- $p/3$ (rounded down) with a minimum node size of 5 as the default for a regression problem.\n",
    "\n",
    "While this is a guideline, Hastie and Tibshirani (authors of Introduction to Statistical Learning and Elements of Statistical Learning) have suggested this as a good guideline in the absence of a rationale to do something different.\n",
    "\n",
    "### Extremely Randomized Trees\n",
    "Adding one further step of randomization yields extremely randomized trees, or _ExtraTrees_. These are trained using bagging (sampling of observations) and the random subspace (sampling of features)method, like in an ordinary random forest, but an additional layer of randomness is introduced. Instead of computing the locally optimal feature/split combination (based on, e.g., information gain or the Gini impurity), for each feature under consideration, a random value is selected for the split. This value is selected from the feature's empirical range (in the tree's training set, i.e., the bootstrap sample), in other words, the top-down splitting in the tree learner is randomized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Guided Practice: Random Forest and ExtraTrees in Scikit Learn (20 min)\n",
    "\n",
    "Scikit Learn implements both random forest and extra trees methods as part of the `ensemble` module.\n",
    "\n",
    "First have a look at the [documentation](http://scikit-learn.org/stable/modules/ensemble.html#forest). (5 min).\n",
    "\n",
    "**Check:** What parameters did you notice? Any questions on those?\n",
    "\n",
    "Let's load the [car dataset](https://archive.ics.uci.edu/ml/machine-learning-databases/car/).\n",
    "\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "df = pd.read_csv('./assets/datasets/car.csv')\n",
    "df.head()\n",
    "```\n",
    "\n",
    "\n",
    "<div>\n",
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>buying</th>\n",
    "      <th>maint</th>\n",
    "      <th>doors</th>\n",
    "      <th>persons</th>\n",
    "      <th>lug_boot</th>\n",
    "      <th>safety</th>\n",
    "      <th>acceptability</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>0</th>\n",
    "      <td>vhigh</td>\n",
    "      <td>vhigh</td>\n",
    "      <td>2</td>\n",
    "      <td>2</td>\n",
    "      <td>small</td>\n",
    "      <td>low</td>\n",
    "      <td>unacc</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>1</th>\n",
    "      <td>vhigh</td>\n",
    "      <td>vhigh</td>\n",
    "      <td>2</td>\n",
    "      <td>2</td>\n",
    "      <td>small</td>\n",
    "      <td>med</td>\n",
    "      <td>unacc</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>2</th>\n",
    "      <td>vhigh</td>\n",
    "      <td>vhigh</td>\n",
    "      <td>2</td>\n",
    "      <td>2</td>\n",
    "      <td>small</td>\n",
    "      <td>high</td>\n",
    "      <td>unacc</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>3</th>\n",
    "      <td>vhigh</td>\n",
    "      <td>vhigh</td>\n",
    "      <td>2</td>\n",
    "      <td>2</td>\n",
    "      <td>med</td>\n",
    "      <td>low</td>\n",
    "      <td>unacc</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>4</th>\n",
    "      <td>vhigh</td>\n",
    "      <td>vhigh</td>\n",
    "      <td>2</td>\n",
    "      <td>2</td>\n",
    "      <td>med</td>\n",
    "      <td>med</td>\n",
    "      <td>unacc</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>\n",
    "</div>\n",
    "\n",
    "This time we will encode the features using a One Hot encoding scheme, i.e. we will consider them as categorical variables. We also need to encode the label using `LabelEncoder`.\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "y = LabelEncoder().fit_transform(df['acceptability'])\n",
    "X = pd.get_dummies(df.drop('acceptability', axis=1))\n",
    "```\n",
    "\n",
    "We would like to compare the performance of the following 4 algorithms:\n",
    "\n",
    "- Decision Trees\n",
    "- Bagging + Decision Trees\n",
    "- Random Forest\n",
    "- Extra Trees\n",
    "\n",
    "Note that in order for our results to be consistent we have to expose the models to exactly the same Cross Validation scheme. Let's start by initializing that.\n",
    "\n",
    "```python\n",
    "from sklearn.cross_validation import cross_val_score, StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, BaggingClassifier\n",
    "\n",
    "cv = StratifiedKFold(y, n_folds=3, shuffle=True, random_state=41)\n",
    "```\n",
    "\n",
    "Now let's initialize a Decision Tree Classifier and evaluate its performance:\n",
    "\n",
    "```python\n",
    "dt = DecisionTreeClassifier(class_weight='balanced')\n",
    "s = cross_val_score(dt, X, y, cv=cv, n_jobs=-1)\n",
    "print \"{} Score:\\t{:0.3} ± {:0.3}\".format(\"Decision Tree\", s.mean().round(3), s.std().round(3))\n",
    "```\n",
    "\n",
    "    Decision Tree Score:\t0.962 ± 0.008\n",
    "\n",
    "\n",
    "### Your turn now:\n",
    "\n",
    "Initialize the following models and check their performance:\n",
    "- Bagging + Decision Trees\n",
    "- Random Forest\n",
    "- Extra Trees\n",
    "\n",
    "You can also create a function to speed up your work..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>buying</th>\n",
       "      <th>maint</th>\n",
       "      <th>doors</th>\n",
       "      <th>persons</th>\n",
       "      <th>lug_boot</th>\n",
       "      <th>safety</th>\n",
       "      <th>acceptability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>small</td>\n",
       "      <td>low</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>small</td>\n",
       "      <td>med</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>small</td>\n",
       "      <td>high</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>med</td>\n",
       "      <td>low</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>vhigh</td>\n",
       "      <td>vhigh</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>med</td>\n",
       "      <td>med</td>\n",
       "      <td>unacc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  buying  maint doors persons lug_boot safety acceptability\n",
       "0  vhigh  vhigh     2       2    small    low         unacc\n",
       "1  vhigh  vhigh     2       2    small    med         unacc\n",
       "2  vhigh  vhigh     2       2    small   high         unacc\n",
       "3  vhigh  vhigh     2       2      med    low         unacc\n",
       "4  vhigh  vhigh     2       2      med    med         unacc"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('./assets/datasets/car.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "y = LabelEncoder().fit_transform(df['acceptability'])\n",
    "X = pd.get_dummies(df.drop('acceptability', axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 2, 2, ..., 2, 1, 3])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    1210\n",
       "0     384\n",
       "1      69\n",
       "3      65\n",
       "dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(y).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "unacc    1210\n",
       "acc       384\n",
       "good       69\n",
       "vgood      65\n",
       "Name: acceptability, dtype: int64"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(df['acceptability']).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_score, StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, BaggingClassifier\n",
    "\n",
    "cv = StratifiedKFold(y, n_folds=3, shuffle=True, random_state=41)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree Score:\t0.964 ± 0.01\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier(class_weight='balanced')\n",
    "s = cross_val_score(dt, X, y, cv=cv, n_jobs=-1)\n",
    "print \"{} Score:\\t{:0.3} ± {:0.3}\".format(\"Decision Tree\", s.mean().round(3), s.std().round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Score:\t0.968 ± 0.006\n"
     ]
    }
   ],
   "source": [
    "dt = BaggingClassifier()\n",
    "s = cross_val_score(dt, X, y, cv=cv, n_jobs=-1)\n",
    "print \"{} Score:\\t{:0.3} ± {:0.3}\".format(\"Bagging\", s.mean().round(3), s.std().round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest with Balanced Classes Score:\t0.949 ± 0.005\n"
     ]
    }
   ],
   "source": [
    "dt = RandomForestClassifier(class_weight='balanced')\n",
    "s = cross_val_score(dt, X, y, cv=cv, n_jobs=-1)\n",
    "print \"{} Score:\\t{:0.3} ± {:0.3}\".format(\"Random Forest with Balanced Classes\", s.mean().round(3), s.std().round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest without Balanced Classes Score:\t0.942 ± 0.004\n"
     ]
    }
   ],
   "source": [
    "dt = RandomForestClassifier()\n",
    "s = cross_val_score(dt, X, y, cv=cv, n_jobs=-1)\n",
    "print \"{} Score:\\t{:0.3} ± {:0.3}\".format(\"Random Forest without Balanced Classes\", s.mean().round(3), s.std().round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extra Trees Score:\t0.954 ± 0.012\n"
     ]
    }
   ],
   "source": [
    "dt = ExtraTreesClassifier()\n",
    "s = cross_val_score(dt, X, y, cv=cv, n_jobs=-1)\n",
    "print \"{} Score:\\t{:0.3} ± {:0.3}\".format(\"Extra Trees\", s.mean().round(3), s.std().round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intro to Boosting (15 min)\n",
    "\n",
    "With bagging and random forests we train models on separate subsets and then combine their prediction. In a sense we are parallelizing the training and then combining (like a map-reduce, for those familiar with the term).\n",
    "\n",
    "_Boosting_ is a different ensemble technique that is _sequential_.\n",
    "\n",
    "![BoostingBagging](./assets/images/BoostingVSBagging.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this class we learned about Random Forests and extremely randomized trees. They are different ways to improve the performance of a weak learner.\n",
    "\n",
    "Some of these methods will perform better in some cases, some better in other cases. For example, decision trees are more nimble and easier to communicate, but have a tendency to overfit. On the other hand, ensemble methods perform better in more complex scenarios, but may become very complicated and harder to explain. (This gets back to our discussion about prediction versus inference - only you and your stakeholders will recognize what balance to strike between these two!)\n",
    "\n",
    "Have a look [here](https://www.wise.io/resources) for a couple of examples from real world startup Wise.io."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ADDITIONAL RESOURCES\n",
    "\n",
    "- [Random Forest on wikipedia](https://en.wikipedia.org/wiki/Random_forest)\n",
    "- [Quora question on Random Forest](https://www.quora.com/How-does-randomization-in-a-random-forest-work?redirected_qid=212859)\n",
    "- [Scikit Learn Ensemble Methods](http://scikit-learn.org/stable/modules/ensemble.html)\n",
    "- [Scikit Learn Random Forest Classifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
