{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "@author: JosephNelson\n",
    "\"\"\"\n",
    "'''\n",
    "Introduction\n",
    "We've discussed overfitting in the context of bias and variance, and we've seen some techniques \n",
    "like regularization that are used to avoid overfitting. In this lesson we'll discuss another method for \n",
    "avoid overfitting that is commonly referred to a the train/test split. The idea is very similar to \n",
    "cross-validation (indeed it is a type of cross-validation) in that we split the dataset into two subsets:\n",
    "a subset to train our model on, and a subset to test our model's predictions on.\n",
    "This serves two useful purposes:\n",
    "We prevent overfitting by not using all the data, and\n",
    "We have some remaining data to evaluate our model.\n",
    "While it may seem like a relatively simple idea, there are some caveats to putting it into practice. \n",
    "For example, if you are not careful it is easy to take a non-random split. Suppose we have salary data on\n",
    "technical professionals that is composed 80% of data from California and 20% elsewhere and is sorted by state.\n",
    "If we split our data into 80% training data and 20% testing data we ight inadvertantly select all the\n",
    "California data to train and all the non-California data to test. In this case we've still overfit on \n",
    "our data set because we did not sufficiently randomize the data.\n",
    "In a situation like this we can use k-fold cross validation, which is the same idea applied to more \n",
    "than two subsets. In particular, we partition our data into k subsets and train on k−1 one of them. \n",
    "Holding the last slice for testing. We can do this for each of the possible k−1 subsets.\n",
    "Demo\n",
    "Let's explore test-training split with some sample datasets.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "% matplotlib inline\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "# Make the plots bigger\n",
    "plt.rcParams['figure.figsize'] = 10, 10\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import seaborn as sns\n",
    "from sklearn import datasets, linear_model\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the diabetes dataset\n",
    "columns = \"age sex bmi map tc ldl hdl tch ltg glu\".split()\n",
    "diabetes = datasets.load_diabetes()\n",
    "df = pd.DataFrame(diabetes.data, columns=columns)\n",
    "y = diabetes.target\n",
    "# Take a look at the data again\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Scikit-learn has a nice function to split a dataset for testing and training called train_test_split.\n",
    "The test_size keyword argument indicates the proportion of the data that should be held over for testing.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training and testing vars\n",
    "X_train, X_test, y_train, y_test = train_test_split(df, y, test_size=0.2)\n",
    "print X_train.shape, y_train.shape\n",
    "print X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# fit a model\n",
    "lm = linear_model.LinearRegression()\n",
    "\n",
    "model = lm.fit(X_train, y_train)\n",
    "predictions = lm.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## The line / model\n",
    "plt.scatter(y_test, predictions)\n",
    "plt.xlabel(\"True Values\")\n",
    "plt.ylabel(\"Predictions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print \"Score:\", model.score(X_test, y_test)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "Now let's try out k-fold cross-validation. Again scikit-learn provides useful functions to do the heavy lifting. The function cross_val_predict returns the predicted values for each data point when it's in the testing slice.\n",
    "'''\n",
    "\n",
    "from sklearn.cross_validation import cross_val_score, cross_val_predict\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform 6-fold cross validation\n",
    "scores = cross_val_score(model, X_train, y_train, cv=6)\n",
    "print \"Cross-validated scores:\", scores\n",
    "print \"Average: \", scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make cross validated predictions\n",
    "predictions = cross_val_predict(model, df, y, cv=6)\n",
    "plt.scatter(y, predictions)\n",
    "accuracy = metrics.r2_score(y, predictions)\n",
    "print \"Cross-Predicted Accuracy:\", accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
