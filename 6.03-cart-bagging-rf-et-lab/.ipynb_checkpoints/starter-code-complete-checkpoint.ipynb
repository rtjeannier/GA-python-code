{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing CARTs with admissions data\n",
    "\n",
    "Using the admissions data from earlier in the course, build CARTs, look at how they work visually, and compare their performance to more standard, parametric models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Install and load the packages required to visually show decision tree branching\n",
    "\n",
    "You will need to first:\n",
    "\n",
    "1. Install `graphviz` with homebrew (on OSX - not sure what linux uses). The command will be `brew install graphviz`\n",
    "- Install `pydotplus` with `pip install pydotplus`\n",
    "- Load the packages as shown below (you may need to restart the kernel after the installations.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REQUIREMENTS:\n",
    "# pip install pydotplus\n",
    "# brew install graphviz\n",
    "\n",
    "# Use graphviz to make a chart of the regression tree decision points:\n",
    "from sklearn.externals.six import StringIO  \n",
    "from IPython.display import Image  \n",
    "from sklearn.tree import export_graphviz\n",
    "import pydotplus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Load in admissions data and other python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/roland/anaconda3/envs/ga-immersive/lib/python2.7/site-packages/statsmodels/compat/pandas.py:56: FutureWarning: The pandas.core.datetools module is deprecated and will be removed in a future version. Please use the pandas.tseries module instead.\n",
      "  from pandas.core import datetools\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "\n",
    "from sklearn.cross_validation import cross_val_score, StratifiedKFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, BaggingClassifier\n",
    "\n",
    "plt.style.use('fivethirtyeight')\n",
    "\n",
    "from ipywidgets import *\n",
    "from IPython.display import display\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "admit = pd.read_csv('assets/admissions.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "admit.head()\n",
    "admit = admit.drop_duplicates()\n",
    "admit.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Create regression and classification X, y data\n",
    "\n",
    "The regression data will be:\n",
    "\n",
    "    Xr = [admit, gre, prestige]\n",
    "    yr = gpa\n",
    "    \n",
    "The classification data will be:\n",
    "\n",
    "    Xc = [gre, gpa, prestige]\n",
    "    yc = admit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>gpa</td>       <th>  R-squared:         </th> <td>   0.151</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.144</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   22.93</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Mon, 24 Jul 2017</td> <th>  Prob (F-statistic):</th> <td>1.09e-13</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>19:30:32</td>     <th>  Log-Likelihood:    </th> <td> -142.11</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   392</td>      <th>  AIC:               </th> <td>   292.2</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   388</td>      <th>  BIC:               </th> <td>   308.1</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>        <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>    <td>    2.6558</td> <td>    0.109</td> <td>   24.433</td> <td> 0.000</td> <td>    2.442</td> <td>    2.870</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>admit</th>    <td>    0.0813</td> <td>    0.040</td> <td>    2.049</td> <td> 0.041</td> <td>    0.003</td> <td>    0.159</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gre</th>      <td>    0.0012</td> <td>    0.000</td> <td>    7.561</td> <td> 0.000</td> <td>    0.001</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>prestige</th> <td>    0.0065</td> <td>    0.019</td> <td>    0.338</td> <td> 0.736</td> <td>   -0.031</td> <td>    0.045</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 5.689</td> <th>  Durbin-Watson:     </th> <td>   1.917</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.058</td> <th>  Jarque-Bera (JB):  </th> <td>   5.654</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.263</td> <th>  Prob(JB):          </th> <td>  0.0592</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.737</td> <th>  Cond. No.          </th> <td>3.70e+03</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                    gpa   R-squared:                       0.151\n",
       "Model:                            OLS   Adj. R-squared:                  0.144\n",
       "Method:                 Least Squares   F-statistic:                     22.93\n",
       "Date:                Mon, 24 Jul 2017   Prob (F-statistic):           1.09e-13\n",
       "Time:                        19:30:32   Log-Likelihood:                -142.11\n",
       "No. Observations:                 392   AIC:                             292.2\n",
       "Df Residuals:                     388   BIC:                             308.1\n",
       "Df Model:                           3                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          2.6558      0.109     24.433      0.000       2.442       2.870\n",
       "admit          0.0813      0.040      2.049      0.041       0.003       0.159\n",
       "gre            0.0012      0.000      7.561      0.000       0.001       0.001\n",
       "prestige       0.0065      0.019      0.338      0.736      -0.031       0.045\n",
       "==============================================================================\n",
       "Omnibus:                        5.689   Durbin-Watson:                   1.917\n",
       "Prob(Omnibus):                  0.058   Jarque-Bera (JB):                5.654\n",
       "Skew:                          -0.263   Prob(JB):                       0.0592\n",
       "Kurtosis:                       2.737   Cond. No.                     3.70e+03\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 3.7e+03. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xr= admit[['admit', 'gre', 'prestige']]\n",
    "yr = admit['gpa'] \n",
    "Xc = admit[['gre', 'gpa','prestige']]\n",
    "yc = admit['admit']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Cross-validate regression and logistic regression on the data\n",
    "\n",
    "Fit a linear regression for the regression problem and a logistic for the classification problem. Cross-validate the R2 and accuracy scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>           <td>gpa</td>       <th>  R-squared:         </th> <td>   0.151</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.144</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   22.93</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Mon, 24 Jul 2017</td> <th>  Prob (F-statistic):</th> <td>1.09e-13</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>19:31:11</td>     <th>  Log-Likelihood:    </th> <td> -142.11</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td>   392</td>      <th>  AIC:               </th> <td>   292.2</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td>   388</td>      <th>  BIC:               </th> <td>   308.1</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     3</td>      <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>        <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>    <td>    2.6558</td> <td>    0.109</td> <td>   24.433</td> <td> 0.000</td> <td>    2.442</td> <td>    2.870</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>admit</th>    <td>    0.0813</td> <td>    0.040</td> <td>    2.049</td> <td> 0.041</td> <td>    0.003</td> <td>    0.159</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gre</th>      <td>    0.0012</td> <td>    0.000</td> <td>    7.561</td> <td> 0.000</td> <td>    0.001</td> <td>    0.001</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>prestige</th> <td>    0.0065</td> <td>    0.019</td> <td>    0.338</td> <td> 0.736</td> <td>   -0.031</td> <td>    0.045</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td> 5.689</td> <th>  Durbin-Watson:     </th> <td>   1.917</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th> <td> 0.058</td> <th>  Jarque-Bera (JB):  </th> <td>   5.654</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>          <td>-0.263</td> <th>  Prob(JB):          </th> <td>  0.0592</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>      <td> 2.737</td> <th>  Cond. No.          </th> <td>3.70e+03</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:                    gpa   R-squared:                       0.151\n",
       "Model:                            OLS   Adj. R-squared:                  0.144\n",
       "Method:                 Least Squares   F-statistic:                     22.93\n",
       "Date:                Mon, 24 Jul 2017   Prob (F-statistic):           1.09e-13\n",
       "Time:                        19:31:11   Log-Likelihood:                -142.11\n",
       "No. Observations:                 392   AIC:                             292.2\n",
       "Df Residuals:                     388   BIC:                             308.1\n",
       "Df Model:                           3                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const          2.6558      0.109     24.433      0.000       2.442       2.870\n",
       "admit          0.0813      0.040      2.049      0.041       0.003       0.159\n",
       "gre            0.0012      0.000      7.561      0.000       0.001       0.001\n",
       "prestige       0.0065      0.019      0.338      0.736      -0.031       0.045\n",
       "==============================================================================\n",
       "Omnibus:                        5.689   Durbin-Watson:                   1.917\n",
       "Prob(Omnibus):                  0.058   Jarque-Bera (JB):                5.654\n",
       "Skew:                          -0.263   Prob(JB):                       0.0592\n",
       "Kurtosis:                       2.737   Cond. No.                     3.70e+03\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "[2] The condition number is large, 3.7e+03. This might indicate that there are\n",
       "strong multicollinearity or other numerical problems.\n",
       "\"\"\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xr = sm.add_constant(Xr)\n",
    "# Note the difference in argument order\n",
    "model = sm.OLS(yr, Xr).fit()\n",
    "predictions = model.predict(Xr)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimization terminated successfully.\n",
      "         Current function value: 0.574528\n",
      "         Iterations 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Logit Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>       <td>admit</td>      <th>  No. Observations:  </th>  <td>   392</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>               <td>Logit</td>      <th>  Df Residuals:      </th>  <td>   388</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>               <td>MLE</td>       <th>  Df Model:          </th>  <td>     3</td>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>          <td>Mon, 24 Jul 2017</td> <th>  Pseudo R-squ.:     </th>  <td>0.07645</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>              <td>19:32:15</td>     <th>  Log-Likelihood:    </th> <td> -225.22</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>converged:</th>           <td>True</td>       <th>  LL-Null:           </th> <td> -243.86</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th> </th>                      <td> </td>        <th>  LLR p-value:       </th> <td>4.001e-08</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>        <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>const</th>    <td>   -3.0647</td> <td>    1.146</td> <td>   -2.675</td> <td> 0.007</td> <td>   -5.311</td> <td>   -0.819</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gre</th>      <td>    0.0022</td> <td>    0.001</td> <td>    2.018</td> <td> 0.044</td> <td> 6.37e-05</td> <td>    0.004</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>gpa</th>      <td>    0.6785</td> <td>    0.331</td> <td>    2.052</td> <td> 0.040</td> <td>    0.030</td> <td>    1.327</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>prestige</th> <td>   -0.5647</td> <td>    0.128</td> <td>   -4.406</td> <td> 0.000</td> <td>   -0.816</td> <td>   -0.314</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                           Logit Regression Results                           \n",
       "==============================================================================\n",
       "Dep. Variable:                  admit   No. Observations:                  392\n",
       "Model:                          Logit   Df Residuals:                      388\n",
       "Method:                           MLE   Df Model:                            3\n",
       "Date:                Mon, 24 Jul 2017   Pseudo R-squ.:                 0.07645\n",
       "Time:                        19:32:15   Log-Likelihood:                -225.22\n",
       "converged:                       True   LL-Null:                       -243.86\n",
       "                                        LLR p-value:                 4.001e-08\n",
       "==============================================================================\n",
       "                 coef    std err          z      P>|z|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "const         -3.0647      1.146     -2.675      0.007      -5.311      -0.819\n",
       "gre            0.0022      0.001      2.018      0.044    6.37e-05       0.004\n",
       "gpa            0.6785      0.331      2.052      0.040       0.030       1.327\n",
       "prestige      -0.5647      0.128     -4.406      0.000      -0.816      -0.314\n",
       "==============================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Xc = sm.add_constant(Xc)\n",
    "# Note the difference in argument order\n",
    "model = sm.Logit(yc, Xc).fit()\n",
    "predictions = model.predict(Xr)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Building regression trees\n",
    "\n",
    "With `DecisionTreeRegressor`:\n",
    "\n",
    "1. Build 4 models with different parameters for `max_depth`: `max_depth=1`, `max_depth=2`, `max_depth=3`, and `max_depth=None`\n",
    "2. Cross-validate the R2 scores of each of the models and compare to the linear regression earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Visualizing the regression tree decisions\n",
    "\n",
    "Use the template code below to create charts that show the logic/branching of your four decision tree regressions from above.\n",
    "\n",
    "#### Interpreting a regression tree diagram\n",
    "\n",
    "- First line is the condition used to split that node (go left if true, go right if false)\n",
    "- `samples` is the number of observations in that node before splitting\n",
    "- `mse` is the mean squared error calculated by comparing the actual response values in that node against the mean response value in that node\n",
    "- `value` is the mean response value in that node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEMPLATE CODE\n",
    "\n",
    "# initialize the output file object\n",
    "dot_data = StringIO() \n",
    "\n",
    "# my fit DecisionTreeRegressor object here is: dtr1\n",
    "# for feature_names i put the columns of my Xr matrix\n",
    "export_graphviz(dtr1, out_file=dot_data,  \n",
    "                filled=True, rounded=True,\n",
    "                special_characters=True,\n",
    "                feature_names=Xr.columns)  \n",
    "\n",
    "graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n",
    "Image(graph.create_png())  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Building classification trees\n",
    "\n",
    "With `DecisionTreeClassifier`:\n",
    "\n",
    "1. Again build 4 models with different parameters for `max_depth`: `max_depth=1`, `max_depth=2`, `max_depth=3`, and `max_depth=None`\n",
    "2. Cross-validate the accuracy scores of each of the models and compare to the logistic regression earlier.\n",
    "\n",
    "Note that now you'll be using the classification task where we are predicting `admit`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Visualize the classification trees\n",
    "\n",
    "The plotting code will be the same as for regression, you just need to change the model you're using for each plot and the feature names.\n",
    "\n",
    "The output changes somewhat from the regression tree chart. Earlier it would give the MSE of that node, but now there is a line called `value` that tells you the count of each class at that node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Using GridSearchCV to find the best decision tree classifier\n",
    "\n",
    "Decision tree regression and classification models in sklearn offer a variety of ways to \"pre-prune\" (by restricting the how many times the tree can branch and what it can use).\n",
    "\n",
    "Measure           | What it does\n",
    "------------------|-------------\n",
    "max_depth         | How many nodes deep can the decision tree go?\n",
    "max_features      | Is there a cut off to the number of features to use?\n",
    "max_leaf_nodes    | How many leaves can be generated per node?\n",
    "min_samples_leaf  | How many samples need to be included at a leaf, at a minimum?  \n",
    "min_samples_split | How many samples need to be included at a node, at a minimum?\n",
    "\n",
    "It is not always best to search over _all_ of these in a grid search, unless you have a small dataset. Many of them while not redundant are going to have very similar effects on your model's fit.\n",
    "\n",
    "Check out the documentation here:\n",
    "\n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html\n",
    "\n",
    "---\n",
    "\n",
    "## Switch over to the college stats dataset for the rest of the lab.\n",
    "\n",
    "We are going to be predicting whether or not a college is public or private. Set up your `X`, `y` variables accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "col = pd.read_csv('/assets/college.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Set up and run the GridSearch on the college data, building the best decision tree classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a bagging classifier using GridSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a Random Forest classifier using GridSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build an ExtraTrees classifier using GridSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compare the best models. Based on GridSearchCV, which is the best model to build? Build it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Print out the \"feature importances\" of this best model.\n",
    "\n",
    "The model has an attribute called `.feature_importances_` which can tell us which features were most important vs. others. It ranges from 0 to 1, with 1 being the most important.\n",
    "\n",
    "An easy way to think about the feature importance is how much that particular variable was used to make decisions. Really though, it also takes into account how much that feature contributed to splitting up the class or reducing the variance.\n",
    "\n",
    "A feature with higher feature importance reduced the criterion (impurity) more than the other features.\n",
    "\n",
    "Below, show the feature importances for each variable predicting private vs. not, sorted by most important feature to least."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
