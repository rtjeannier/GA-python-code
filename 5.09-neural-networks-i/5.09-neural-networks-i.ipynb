{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![](https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png) An Overview of Neural Networks\n",
    "\n",
    "# Learning Objectives:\n",
    "- Describe the structure of neural networks\n",
    "- Explain how neural networks capture relationships in our data\n",
    "- Code a neural network by hand!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's talk about neural networks!\n",
    "\n",
    "![](./assets/network1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Biological \"Inspiration\"\n",
    "\n",
    "A neuron is the means by which electronic signals are transmitted through the human brain.\n",
    "\n",
    "![](./assets/neuron2.png)\n",
    "\n",
    "Neurons run throughout our nervous system and are connected in an enormous, tangled web that is difficult to understand. Electronic signals transmit through neurons, stopping short in some cases and continuing in others.\n",
    "\n",
    "![](./assets/neuron1.jpg)\n",
    "\n",
    "While neural networks aren't really like the neuronal system in our body, they're good visuals to consider and do have some analogous features.\n",
    "\n",
    "Much like neurons are the building blocks of the nervous system, the **perceptron** is the building block of a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Perceptron\n",
    "\n",
    "There are five main parts to the perceptron:\n",
    "\n",
    "1. Input Layer\n",
    "2. Weights\n",
    "3. Activation Function\n",
    "4. Bias\n",
    "5. Output\n",
    "\n",
    "![](./assets/perceptron.gif)\n",
    "\n",
    "We'll use this diagram to walk through the different pieces of the perceptron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input Layer\n",
    "![](./assets/perceptron.gif)\n",
    "\n",
    "On the left-hand side above, there are four inputs. I'll call them $x_1$, $x_2$, $x_3$, and $x_n$, as this will be more common beyond this particular image. We call this the **input layer**.\n",
    "\n",
    "This input layer is the set of independent variables we read into our model. If we want to predict commute time, we might include number of Metro stops, whether or not it is raining, and distance from GA in miles as some of our input variables. An observation's values would be passed in here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weights\n",
    "![](./assets/perceptron.gif)\n",
    "\n",
    "On each arrow, there is a weight, denoted $w_1$, $w_2$, $w_3$, and $w_n$. We call these our **weights** or **synaptic weights**.\n",
    "\n",
    "The weights are similar to our coefficients. We usually seek to estimate these. We might specify starting weights and then iterate to find the best values of the weights. (We'll cover that later.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation Function\n",
    "![](./assets/perceptron.gif)\n",
    "\n",
    "Near the middle, there is a function denoted $\\sigma(\\cdot)$. We call this our **activation function**.\n",
    "\n",
    "In a biological neuron, the electrical impulses gather and has to reach some threshold called \"action potential\" in order for the neuron to \"fire.\" Either the neural reaches this \"action potential\" and \"fires,\" or it does not reach \"action potential\" and it does not fire.\n",
    "\n",
    "In artificial neural networks, we have to specify this activation function and it need not be \"all or nothing.\" Our choice of activation function is important, as it will identify what signals get through and ultimately determines the output.\n",
    "\n",
    "![](./assets/activation_function.png)\n",
    "\n",
    "Another example is RELU (rectified linear unit). This ensures that our output is nonnegative by calculating $\\sigma(z) = \\max(z,0)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bias\n",
    "![](./assets/perceptron.gif)\n",
    "\n",
    "Inside the $\\sigma$, we see $b +$ the inner product of our weights and inputs. This value $b$ is referred to as the **bias**. We can specify some bias in conjunction with our activation functions to achieve certain results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output\n",
    "![](./assets/perceptron.gif)\n",
    "\n",
    "On the right-hand side, we see an arrow extending forward. The result of $\\sigma(b+\\sum_{i=1}^nx_iw_i)$ is our **output**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Practice Problems:\n",
    "\n",
    "1. Suppose I want to take in $n$ binary variables and return the `and` operator. How could I specify my perceptron?\n",
    "\n",
    "2. Suppose I want to take in $n$ binary variables and return the `or` operator. How could I specify my perceptron?\n",
    "\n",
    "3. Suppose I want to take in $n$ binary variables and return the `xor` operator. How could I specify my perceptron?\n",
    "\n",
    "4. Suppose I want to take in $n$ quantitative variables and return the sum of all positive inputs. How could I specify my perceptron?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's Manually Set Up a Network!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "input_data = np.array([30,2])\n",
    "\n",
    "weights = { 'node_0': np.array([2,4]),\n",
    "            'node_1': np.array([-1,1]),\n",
    "            'output': np.array([2,1])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate node 0 value: node_0_value\n",
    "node_0_value = (input_data * weights['node_0']).sum()\n",
    "\n",
    "# Calculate node 1 value: node_1_value\n",
    "node_1_value = (input_data * weights['node_1']).sum()\n",
    "\n",
    "# Put node values into array: hidden_layer_outputs\n",
    "hidden_layer_outputs = np.array([node_0_value, node_1_value])\n",
    "\n",
    "# Calculate output: output\n",
    "output = (hidden_layer_outputs*weights['output']).sum()\n",
    "\n",
    "# Print output\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def relu(input):\n",
    "    '''Define your relu activation function here'''\n",
    "    # Calculate the value for the output of the relu function: output\n",
    "    output = max(input, 0)\n",
    "    \n",
    "    # Return the value just calculated\n",
    "    return(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate node 0 value: node_0_output\n",
    "node_0_input = (input_data * weights['node_0']).sum()\n",
    "node_0_output = relu(node_0_input)\n",
    "\n",
    "# Calculate node 1 value: node_1_output\n",
    "node_1_input = (input_data * weights['node_1']).sum()\n",
    "node_1_output = relu(node_1_input)\n",
    "\n",
    "# Put node values into array: hidden_layer_outputs\n",
    "hidden_layer_outputs = np.array([node_0_output, node_1_output])\n",
    "\n",
    "# Calculate model output (do not apply relu)\n",
    "model_output = (hidden_layer_outputs * weights['output']).sum()\n",
    "\n",
    "# Print model output\n",
    "print(model_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define predict_with_network()\n",
    "def predict_with_network(input_data_row, weights):\n",
    "\n",
    "    # Calculate node 0 value\n",
    "    node_0_input = (input_data_row * weights['node_0']).sum()\n",
    "    node_0_output = relu(node_0_input)\n",
    "\n",
    "    # Calculate node 1 value\n",
    "    node_1_input = (input_data_row * weights['node_1']).sum()\n",
    "    node_1_output = relu(node_1_input)\n",
    "\n",
    "    # Put node values into array: hidden_layer_outputs\n",
    "    hidden_layer_outputs = np.array([node_0_output, node_1_output])\n",
    "    \n",
    "    # Calculate model output\n",
    "    input_to_final_layer = (hidden_layer_outputs * weights['output']).sum()\n",
    "    model_output = relu(input_to_final_layer)\n",
    "    \n",
    "    # Return model output\n",
    "    return(model_output)\n",
    "\n",
    "\n",
    "# Create empty list to store prediction results\n",
    "results = []\n",
    "for input_data_row in input_data:\n",
    "    # Append prediction to results\n",
    "    results.append(predict_with_network(input_data_row, weights))\n",
    "\n",
    "# Print results\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setting Up a Deeper Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "input_data = np.array([-1,4])\n",
    "\n",
    "weights = { 'node_0_0': np.array([3,3]),\n",
    "            'node_0_1': np.array([3,3]),\n",
    "            'node_1_0': np.array([3,3]),\n",
    "            'node_1_1': np.array([3,3]),\n",
    "            'output': np.array([2,-1])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_with_network(input_data):\n",
    "    # Calculate node 0 in the first hidden layer\n",
    "    node_0_0_input = (input_data * weights['node_0_0']).sum()\n",
    "    node_0_0_output = relu(node_0_0_input)\n",
    "\n",
    "    # Calculate node 1 in the first hidden layer\n",
    "    node_0_1_input = (input_data * weights['node_0_1']).sum()\n",
    "    node_0_1_output = relu(node_0_1_input)\n",
    "\n",
    "    # Put node values into array: hidden_0_outputs\n",
    "    hidden_0_outputs = np.array([node_0_0_output, node_0_1_output])\n",
    "    \n",
    "    # Calculate node 0 in the second hidden layer\n",
    "    node_1_0_input = (hidden_0_outputs*weights['node_1_0']).sum()\n",
    "    node_1_0_output = relu(node_1_0_input)\n",
    "\n",
    "    # Calculate node 1 in the second hidden layer\n",
    "    node_1_1_input = (hidden_0_outputs*weights['node_1_1']).sum()\n",
    "    node_1_1_output = relu(node_1_1_input)\n",
    "\n",
    "    # Put node values into array: hidden_1_outputs\n",
    "    hidden_1_outputs = np.array([node_1_0_output, node_1_1_output])\n",
    "\n",
    "    # Calculate model output: model_output\n",
    "    model_output = (hidden_1_outputs*weights['output']).sum()\n",
    "    \n",
    "    # Return model_output\n",
    "    return(model_output)\n",
    "\n",
    "output = predict_with_network(input_data)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_with_network(input_data):\n",
    "    # Calculate node 0 in the first hidden layer\n",
    "    node_0_0_input = (input_data * weights['node_0_0']).sum()\n",
    "    node_0_0_output = relu(node_0_0_input)\n",
    "\n",
    "    # Calculate node 1 in the first hidden layer\n",
    "    node_0_1_input = (input_data * weights['node_0_1']).sum()\n",
    "    node_0_1_output = relu(node_0_1_input)\n",
    "\n",
    "    # Put node values into array: hidden_0_outputs\n",
    "    hidden_0_outputs = np.array([node_0_0_output, node_0_1_output])\n",
    "\n",
    "    # Calculate node 0 in the second hidden layer\n",
    "    node_1_0_input = (hidden_0_outputs * weights['node_1_0']).sum()\n",
    "    node_1_0_output = relu(node_1_0_input)\n",
    "\n",
    "    # Calculate node 1 in the second hidden layer\n",
    "    node_1_1_input = (hidden_0_outputs * weights['node_1_1']).sum()\n",
    "    node_1_1_output = relu(node_1_1_input)\n",
    "\n",
    "    # Put node values into array: hidden_1_outputs\n",
    "    hidden_1_outputs = np.array([node_1_0_output, node_1_1_output])\n",
    "    \n",
    "    # Calculate output here: model_output\n",
    "    model_output = (hidden_1_outputs * weights['output']).sum()\n",
    "    \n",
    "    # Return model_output\n",
    "    return(model_output)\n",
    "\n",
    "output = predict_with_network(input_data)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From the Perceptron to a Neural Network\n",
    "\n",
    "Jumping from perceptrons to neural networks is quite straightforward. The outputs of a perceptron become the inputs to new perceptrons.\n",
    "\n",
    "![](./assets/network2.png)\n",
    "\n",
    "- We have six independent variables.\n",
    "- We have, as always, one input layer and one output layer.\n",
    "- We have two hidden layers. This can and will vary substantially from problem to problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How do Neural Networks Work?\n",
    "\n",
    "1. We specify the architecture of our neural network. (Much like model building, this is based on our data and assumptions, and we get better with practice.)\n",
    "2. We decide how many `epochs` $n$ we want to run and how many `batches` $k$ go in an epoch.\n",
    "3. Split training data into $k$ batches.\n",
    "4. Feed first batch into neural network.\n",
    "5. Calculate error and update weights/bias accordingly.\n",
    "6. Feed next batch into neural network.\n",
    "7. Repeat steps 5 and 6 until all $k$ batches have gone through exactly once. This ends the epoch.\n",
    "8. Repeat steps 4-7 until we have completed $n$ epochs.\n",
    "9. Make adjustments as necessary and/or use model for prediction.\n",
    "\n",
    "This glosses over many, many of the details... but provides a broad picture to what is occurring."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check**: What are we worried about?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Resources\n",
    "\n",
    "- Comprehensive list activation functions [StackExchange](http://stats.stackexchange.com/questions/115258/comprehensive-list-of-activation-functions-in-neural-networks-with-pros-cons)\n",
    "- Why convolutional neural networks for images [cs231 - Stanford](http://cs231n.github.io/convolutional-networks/)\n",
    "- The University of Toronto's neural network [Coursera](https://www.coursera.org/learn/neural-networks) is regarded to be among the best\n",
    "- Deep Learning online learning: [Neural Networks and Deep Learning](http://neuralnetworksanddeeplearning.com/chap1.html)\n",
    "- The [Neural Network Zoo](http://www.asimovinstitute.org/neural-network-zoo/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
