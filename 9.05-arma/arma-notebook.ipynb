{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![](https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png) Intro to ARMA Models\n",
    "Week 9 \n",
    "\n",
    "### LEARNING OBJECTIVES\n",
    "*After this lesson, you will be able to:*\n",
    "- Model and predict from time series data using AR or ARMA models\n",
    "- Code those models in `statsmodels`\n",
    "\n",
    "### STUDENT PRE-WORK\n",
    "*Before this lesson, you should already be able to:*\n",
    "- Prior definition and Python functions for moving averages and autocorrelation\n",
    "- Prior exposure to linear regression with discussion of coefficients and residuals\n",
    "- `pip install statsmodels` (should be included with Anaconda)\n",
    "\n",
    "### LESSON GUIDE\n",
    "| TIMING  | TYPE  | TOPIC  |\n",
    "|:-:|---|---|\n",
    "| 5 min  | [Opening](#opening)  | What are Time Series Models?  |\n",
    "| 15 min  | [Introduction](#intro-tsmprops)  | Properties of Time Series Models  |\n",
    "| 10 min  | [Exercise](#exercise)  | Investigating Timeseries Data using Google  |\n",
    "| 5 min  | [Introduction](#intro-ar)  | AR Models  |\n",
    "| 5 min  | [Introduction](#intro-ma)  | MA Models  |\n",
    "| 5 min  | [Introduction](#intro-arma)  | ARMA Models  |\n",
    "| 15 min  | [Demo/Codealong](#demo1)  | Demo/Codealong: Timeseries EDA in `statsmodels`  |\n",
    "| 15 min  | [Demo/Codealong](#demo2)  | Demo/Codealong: AR, MA and ARMA models in `statsmodels`  |\n",
    "| 5 min  | [Conclusion](#conclusion)  | Recap  |\n",
    "\n",
    "---\n",
    "<a name=\"opening\"></a>\n",
    "## Opening: What are Time Series Models? \n",
    "\n",
    "In the last class, we focused on exploring time-series data and common statistics for time-series analysis. In this class, we'll advance those techniques to show how to predict or forecast from time series data.\n",
    "\n",
    "If we have a sequence of values (a time series), we will use the techniques in this class to predict a future value. For example, we may want to predict the number of sales in a future month.\n",
    "\n",
    "### Time Series Models\n",
    "\n",
    "Time series models are models that will be used to predict a future value in the time-series. Like other predictive models, we will use prior history to predict the future! Unlike previous models, we will use the _outcome_ variables from earlier in time as the _inputs_ for prediction.\n",
    "\n",
    "While most of the previous lesson was focused on REFINING time-series data using descriptive statistics or visualization to identify patterns, this class will be focused on BUILDING models for prediction.\n",
    "\n",
    "<a name=\"intro-tsmprops\"></a>\n",
    "## Introduction: Properties of Time Series Models \n",
    "\n",
    "### Training and Test sets\n",
    "\n",
    "Since there is a time component to our data, we **cannot choose training and test examples at random.** As we are attempting to predict _future_ values, we must train exclusively on values from earlier (in time) in our data and then test our values at the end of data period.\n",
    "\n",
    "### Properties for time-series prediction\n",
    "In our last class we saw a few statistics for analyzing time series. We looked at moving averages to evaluate the local behavior of the time series.\n",
    "\n",
    "**Check:** Recall the definition for moving average - what is its purpose?\n",
    "\n",
    "> Answer: A _moving average_ is an average of _k_ surrounding data points in time.\n",
    "\n",
    " ![](./assets/images/single_moving_avg_fit.gif)\n",
    "\n",
    "We also looked at autocorrelation to compute the relationship of the data with prior values.\n",
    "\n",
    "**Check:** Recall the definition for autocorrelation - what is its purpose?\n",
    "\n",
    "\n",
    "> _Autocorrelation_ is how correlated a variable is with itself. Specifically, how related variables from earlier in time are with variables from later in time.\n",
    "\n",
    "![](./assets/images/autocorrelation.gif)\n",
    "\n",
    "Typically, for a high-quality model, we require some autocorrelation in our data. We can compute autocorrelation at various lag values to determine how far back in time we need to go.\n",
    "\n",
    "### Stationarity\n",
    "\n",
    "Many models make an assumption of _stationarity_, which means assuming the mean and variance of our values is the same throughout.\n",
    "\n",
    "This means that while the values (of sales, for example) may shift up and down over time, the _mean value_ of sales is constant, as well as the variance (i.e. there aren't many dramatic swings up or down).\n",
    "\n",
    "As always, these assumptions may not represent real-world data, which we must be aware of when breaking the assumptions of our model for others! For example, typical stock market performance is not stationary. In this plot of Dow Jones performance since 1986, the mean is clearly increasing over time.\n",
    "\n",
    "![](./assets/images/dow-jones.png)\n",
    "\n",
    "Below are simulated examples from \"Investopedia\" of non-stationary time-series and why they might occur:\n",
    "\n",
    "![](./assets/images/nonstationary.gif)\n",
    "\n",
    "Often, if these assumptions don't hold we can alter our data to make them true. Two common methods are _detrending_ and _differencing_.\n",
    "\n",
    "_Detrending_ would remove any major trends in our data. We could do this in many ways, but the simplest way is to fit a line to the trend, then make a new series of the difference between the line and the true series.\n",
    "\n",
    "For example, in iPhone google searches, there is a clear upward (non-stationary) trend:\n",
    "\n",
    "![](./assets/images/google-iphone.png)\n",
    "\n",
    "If we fit a line to this data first, we can create a new series that is the difference between the true number of searches and the predicted searches. We can then fit a time-series model to this difference.\n",
    "\n",
    "Below is an example looking at U.S. housing prices over time. Clearly, there is a trend upward. This makes the time-series non-stationary, as the mean home price is increasing. The line fit through it represents the trend.\n",
    "\n",
    "The bottom figure is the \"detrended\" data, where each data point is transformed by subtracting off the value of the trend line at that point, and shifting by the original starting value. This data now has a fixed mean and may be easier to model.\n",
    "\n",
    "![](./assets/images/detrend.gif)\n",
    "\n",
    "This pattern is similar to mean-scaling our features in earlier models with `StandardScaler`.\n",
    "\n",
    "A simpler but related method is _differencing_. This is very closely related to the `diff` function we saw in the last class.\n",
    "\n",
    "Instead of predicting the (non-stationary) series, we can predict the difference between two consecutive values. We will see that the **ARIMA** model incorporates this approach.\n",
    "\n",
    "<a name=\"exercise\"></a>\n",
    "## Exercise: Investigating Timeseries Data using Google \n",
    "\n",
    "Non-stationary data is the most common type of data, since almost any interesting dataset is non-stationary. Can you think of some datasets that are stationary? Let's open trends.google.com and investigate some time series visualizations of google searches. Students, propose topics that you think would have a stationary time series. Was your assumption correct?\n",
    "\n",
    "# Timeseries Modeling\n",
    "\n",
    "In the rest of this lesson, we are going to build up to the **ARIMA** time-series model. This models combines the ideas of differencing and two models we will see below: **AR** or autoregressive models and **MA** or moving average models.\n",
    "\n",
    "<a name=\"intro-ar\"></a>\n",
    "## AR Models \n",
    "\n",
    "**Autoregressive (AR) models** are those are that use data from previous time-points to predict the next time-point. These are very similar to previous regression models, except as input - we'll take some previous outcome.\n",
    "\n",
    "If we are attempting to predict weekly sales, we'll use sales from a previous week as our input. Typically, AR models are noted AR(p), where _p_ indicates the number of previous time points to incorporate, with AR(1) being the most common.\n",
    "\n",
    "In an autoregressive model, similar to standard regression, we'll learn regression coefficients, where the inputs or features are the previous _p_ values. Therefore, we will learn _p_ coefficients or \\beta values.\n",
    "\n",
    "If we have a time series of sales per week, $y_i$, we can regress each $y_i$ from the last $p$ values.\n",
    "\n",
    "$y_i = intercept + \\beta_1 * y_(i-1) + \\beta_2 * y_(i-2) + ... + \\beta_p * y_(i-p) + random_error$\n",
    "\n",
    "As with standard regression, our model assumes that each outcome variable is a linear combination of the inputs and a random error term.  \n",
    "\n",
    "For AR(1) models, we will learn a _single_ coefficient. This coefficient will tell us the relationship between the previous value and the next one. A value > 1 would indicate a growth over previous values.\n",
    "\n",
    "Note: This would typically represent non-stationary data, since if we compound the increase then the values would be continually increasing.\n",
    "\n",
    "Values between 1 and -1 represent increasing and decreasing patterns, respectively. As with other linear models, interpretation becomes more complex as we add more factors; in other words, as we go from AR(1) to AR(2) since we begin to have significant _multi-collinearity_.\n",
    "\n",
    "Recall, _autocorrelation_ is the correlation of a value with itself. We compute correlation with values _lagged_ behind. A model with high-correlation implies that the data is highly dependent on previous values and an autoregressive model would perform well.\n",
    "\n",
    "Autoregressive models are useful for learning falls or rises in our series. This will weight together the last few values to make a future prediction. Typically, this model type is useful for small-scale trends, such as an increase in demand or a change in tastes that will gradually increase or decrease the series.\n",
    "\n",
    "\n",
    "\n",
    "<a name=\"intro-ma\"></a>\n",
    "## MA Models \n",
    "\n",
    "**Moving average models**, as opposed to autoregressive models, do not take the previous outputs (or values) as inputs, but instead take the previous error terms. We will attempt to predict the next value based on the overall average and how incorrect our previous predictions were.\n",
    "\n",
    "This model is useful for handling specific or abrupt changes in a system. If we consider that autoregressive models are slowly incorporating changes in the system by combining previous values, moving average models use our previous errors.\n",
    "\n",
    "Using these as inputs helps model sudden changes by directly incorporating the prior error. This is useful for modeling a sudden occurrence - like something going out of stock affecting sales or a sudden rise in popularity.\n",
    "\n",
    "As in autoregressive models, we have an order term, _q_, and we refer to our model as $MA(q)$.  This moving average model is dependent on the last _q_ errors.\n",
    "\n",
    "If we have a time series of sales per week, $y_i$, we can regress each $y_i$ from the last $q$ error terms.\n",
    "\n",
    "$y_i = \\mu + \\beta_1 * \\epsilon_i + ... \\beta_q * \\epsilon_q$\n",
    "\n",
    "Of course, we don't have the errors terms when we start - where do they come from?\n",
    "\n",
    "This requires a more complex fitting procedure than we have seen, where we iteratively fit a model (perhaps with random error terms), compute the errors and then refit, over and over again.\n",
    "\n",
    "We'll include the mean of the time series and that is why we call this a moving average, as we assume the model takes the mean value of a series and randomly jumps around it.\n",
    "\n",
    "With this model, we'll learn _q_ coefficients. In an MA(1) model, we learn one coefficient where this value indicates the impact of our previous error on our next prediction.\n",
    "\n",
    "\n",
    "**Check:** What are the AR and MA terms, and how are they defined/tuned?\n",
    "\n",
    "<a name=\"intro-arma\"></a>\n",
    "## ARMA Models (5 min)\n",
    "\n",
    "Another stepping stone to **ARIMA** models are **ARMA** models.\n",
    "\n",
    "_ARMA_, pronounced 'R-mah', models combine the autoregressive models and moving averages. For an ARMA model, we specify two model settings `p` and `q`, which correspond to combining an AR(p) model with an MA(q) model.\n",
    "\n",
    "An ARMA(p, q) model is simply a combination (sum) of an AR(p) and MA(q) model.\n",
    "\n",
    "Incorporating both models allows us to mix two types of effects.\n",
    "\n",
    "Autoregressive models slowly incorporate changes in preferences, tastes, and patterns. Moving average models base their prediction not on the prior value but the prior error, allowing us to correct sudden changes based on random events - supply, popularity spikes, etc.\n",
    "\n",
    "**Check:** Recall the use cases for ARMA models. What is the goal of ARMA modeling?\n",
    "\n",
    "\n",
    "- AR: AutoRegressive process\n",
    "- MA: Moving Average process\n",
    "- ARMA: Model using a combination of Autorgressive and Moving Average terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"demo1\"></a>\n",
    "## Demo: Time Series EDA in `statsmodels` \n",
    "\n",
    "To explore time series models, we will continue with the Rossmann sales data.  This dataset has sales data for sales at every Rossmann store for a 3-year period, as well as indicators of holidays and basic store information.\n",
    "\n",
    "In the last class, we saw that we would plot the sales data at a particular store to identify how the sales changed over time.  Additionally, we computed autocorrelation for the data at varying lag periods. This helps us identify if previous timepoints are predictive of future data and which time points are most important - the previous day? week? month?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Load the data and set the DateTime index\n",
    "data = pd.read_csv('./assets/datasets/rossmann.csv', skipinitialspace=True)\n",
    "\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data.set_index('Date', inplace=True)\n",
    "\n",
    "# Filter to Store 1\n",
    "store1_data = data[data.Store == 1]\n",
    "\n",
    "# Filter to open days\n",
    "store1_open_data = store1_data[store1_data.Open==1]\n",
    "\n",
    "# Plot the sales over time\n",
    "store1_open_data[['Sales']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# Load the data and set the DateTime index\n",
    "data = pd.read_csv('assets/datasets/rossmann.csv', skipinitialspace=True)\n",
    "\n",
    "data['Date'] = pd.to_datetime(data['Date'])\n",
    "data.set_index('Date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter to Store 1\n",
    "store1_data = data[data.Store == 1]\n",
    "\n",
    "# Filter to open days\n",
    "store1_open_data = store1_data[store1_data.Open==1]\n",
    "\n",
    "# Plot the sales over time\n",
    "store1_open_data[['Sales']].plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# **Check** Compute the autocorrelation of Sales in Store 1 for lag 1 and 2. \n",
    "# Will we be able to use a predictive model - particularly an autoregressive one?\n",
    "\n",
    "print store1_data.Sales.autocorr(lag=1) \n",
    "print store1_data.Sales.autocorr(lag=2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do see some minimal correlation in time, implying an AR model can be useful. An easier way to diagnose this may be to plot many autocorrelations at once."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas.tools.plotting import autocorrelation_plot\n",
    "\n",
    "autocorrelation_plot(store1_data.Sales)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Check**: What does the resulting plot show?\n",
    "\n",
    "> Answer: This shows a typical pattern of an autocorrelation plot - it should decrease to 0 as lag increases! However, it's hard to observe exactly what the values are.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the ACF using `statsmodels` and see what we can observe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "\n",
    "plot_acf(store1_data.Sales, lags=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we observe autocorrelation at 10 lag values. 1 and 2 are what we saw before. This implies a small, but limited impact based on the last few values, suggesting that an autoregressive model might be useful.\n",
    "\n",
    "**Check**: We also observe a larger spike at 7 - what does that mean?\n",
    "\n",
    "> That's the amount of days in a week!\n",
    "\n",
    "If we observed a handful of randomly distributed spikes - that would imply a MA model may be useful. This is because those random spikes suggest that at some point in time, something changed in the world and all values are shifted up down from there in a fixed way.\n",
    "\n",
    "That may be the case here, but if we expand the window we can see that the spikes occur regularly at 7 days windows. This means we have a weekly cycle!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acf(store1_data.Sales, lags=25)\n",
    "\n",
    "# Let's start by investigating AR models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"demo2\"></a>\n",
    "## Demo: AR, MA and ARMA models in `statsmodels` (15 mins)\n",
    "\n",
    "To explore AR and ARMA models, we will use `sm.tsa.ARMA`. \n",
    "\n",
    "**Check:** What is the definition of an ARMA model?\n",
    "\n",
    "> Answer: An ARMA model is a combination of autoregressive and moving average models.\n",
    "\n",
    "We can train an autoregressive model by turning off the moving average component (setting q = 0).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.tsa.arima_model import ARMA\n",
    "\n",
    "store1_sales_data = store1_open_data[['Sales']].astype(float)\n",
    "model = ARMA(store1_sales_data, (1, 0)).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reminder: Autoregressive (AR) models use data from previous time-points to predict the next time-point. These are essentially regression models where the predictors are previous timepoints of the outcome.\n",
    "\n",
    "Typically, AR models are denoted AR(p), where p indicates the number of previous time points to incorporate. AR(1) is the most common.\n",
    "In an autoregressive model we learn regression coefficients on the features that are the previous p values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By passing the `(1, 0)` in the second argument, we are fitting an ARMA model as ARMA(p=1, q=1). Remember, an ARMA(p, q) model is AR(p) + MA(q). This means that an ARMA(1, 0) is the same as an AR(1) model.\n",
    "\n",
    "In this AR(1) model we learn an intercept value, or base sales values. Additionally, we learn a coefficient that tells us how to include the last sales values. In this case, we take the intercept of ~4700 and add in the previous months sales * 0.68.\n",
    "\n",
    "Note the coefficient here does not match the lag 1 autocorrelation - implying the the data is not stationary.\n",
    "\n",
    "We can learn an AR(2) model, which regresses each sales value on the last two, with the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ARMA(store1_sales_data, (2, 0)).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we learn two coefficients, which tells us the effect of the last two sales values on current sales. To make a sales prediction for a future month, we would combine the last two months of sales with the weights or coefficients learned.\n",
    "\n",
    "While this model may be able to better model the series, it may be more difficult to interpret.\n",
    "\n",
    "To start to diagnose the model, we want to look at the _residuals_.\n",
    "\n",
    "**Check:** What are residuals? In linear regression, what did we expect of residuals?\n",
    "\n",
    "_Residuals_ are the errors of the model, or a measure of how off our prior predictions were.\n",
    "\n",
    "What we ideally want are randomly distributed errors that are fairly small. If the errors are large then clearly that would be problematic. If the errors have a pattern, particularly over time, then we have overlooked something in the model or certain periods of time are different than the rest of the dataset.\n",
    "\n",
    "We can plot the residuals as below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.resid.plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we saw large spikes at the end of each year, indicating that our model does not account for holiday spikes. Of course, our models are only related to the last few values in the time series, and don't take into account the longer seasonal pattern.\n",
    "\n",
    "We can also plot the autocorrelations of the residuals. In an ideal model, these would all nearly be 0 and hopefully random.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_acf(model.resid, lags=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This aspect is also troubling - the autocorrelation plot shows a clear pattern where errors are increasing and decreasing every week.\n",
    "\n",
    "To expand this AR model to a ARMA model, we can include the moving average component as well.\n",
    "\n",
    "\n",
    "Moving average models take previous error terms as inputs. They predict the next value based on deviations from previous predictions. This can be useful for modeling a sudden occurrence - like something going out of stock affecting sales or a sudden rise in popularity.\n",
    "\n",
    "As in autoregressive models, we have an order term, q, and we refer to our model as MA(q). This moving average model is dependent on the last q errors. If we have a time series of sales per week, $y_i$, we can regress each $y_i$ on the last q error terms.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ARMA(store1_sales_data, (1, 1)).fit()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now we learn two coefficients, one for the AR(1) component and one for the MA(1)\n",
    "\n",
    "Remember this is an AR(1) + MA(1) model. So the AR coefficient represents dependency on the last value and the MA component represents any spikes independent of the last value.\n",
    "\n",
    "The coefficients here are 0.69 for the AR component and -0.03 for the MA component. The AR coefficient is the same as before (decreasing values) and the MA component is fairly small (which we should have expected from the autocorrelation plots).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we can forecast our model:\n",
    "\n",
    "predicts = model.forecast(steps=7)\n",
    "\n",
    "# What happens as we increase our steps?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result of the forecast() function is an array containing the forecast value, the standard error of the forecast, and the confidence interval information.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  There's one part we haven't discussed: How do we deal with non-stationary data?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name=\"conclusion\"></a>\n",
    "\n",
    "## Recap\n",
    "- Timeseries models use previous values to predict future values, also known as forecasting.\n",
    "- AR and MA model are simple models on previous values or previous errors respectively.\n",
    "- ARMA combines these two types of models to account for both local shifts (due to AR models) and abrupt changes (MA models)\n",
    "- None of these models perform very well for data that has lots of random variation - for example, this isn't very useful with searches or sales that tend to increase in short bursts.\n",
    "\n",
    "***\n",
    "\n",
    "\n",
    "### ADDITIONAL RESOURCES\n",
    "- [Time Series Analysis in Python with statsmodels](http://conference.scipy.org/proceedings/scipy2011/pdfs/statsmodels.pdf)\n",
    "- [Investopedia: Stationarity](http://www.investopedia.com/articles/trading/07/stationary.asp)\n",
    "- [Google Search Terms predict market movements](https://www.quantopian.com/posts/google-search-terms-predict-market-movements)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
