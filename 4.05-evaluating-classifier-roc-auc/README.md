# ![Logo](https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png) Evaluating Classifiers: Confusion Matrix, AUC-ROC


We'll be learning how to evaluate classification models. At the conclusion of this lesson, you will know:
- What true positives, false positives, true negatives, and false negatives are
- What a confusion matrix is, and how to apply its use
- What receiver operating characteristic (ROC) curves are, and why they're useful
- How to apply ROC curves


## Resources

- An introduction to [Confusion Matrix terminology](http://www.dataschool.io/simple-guide-to-confusion-matrix-terminology/)
- A deeper [Introduction to ROC](http://people.inf.elte.hu/kiss/13dwhdm/roc.pdf)
- Interactive [playing with ROC curves](http://www.navan.name/roc/)
- Data School's video and transcript on [ROC/AUC](http://www.dataschool.io/roc-curves-and-auc-explained/)
- Watch Rahul Patwari's [video](https://www.youtube.com/watch?v=21Igj5Pr6u4) on ROC curves
- An excellent [Stack Overflow discussion](https://stats.stackexchange.com/questions/132777/what-does-auc-stand-for-and-what-is-it) on ROC-AUC and Confusion Matrices
- [Precision and Recal Formulas on Wikipedia](https://en.wikipedia.org/wiki/Precision_and_recall#Definition_.28classification_context.29)
- [Precision, recall, confusion matrix for multiclass problems](http://text-analytics101.rxnlp.com/2014/10/computing-precision-and-recall-for.html)

### Code
- [Sklearn confusion matrix](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html)
- [Visualizing confusion matrix in Sklearn](http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py)
- [Sklearn ROC-AUC score](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html)
